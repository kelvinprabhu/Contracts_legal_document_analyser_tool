{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ff7ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kelvin/miniconda3/envs/deep-gpu/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! üëã How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Import Azure OpenAI\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "# Initialize LLM with explicit endpoint + key\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=\"neostats_hackathon_api_v1\",\n",
    "    model=\"gpt-oss-120b\",                         \n",
    "    temperature=0,\n",
    "    api_version=\"2024-05-01-preview\",             \n",
    "    api_key=\"Azure_Open_API\",                      # Replace with your actual key\n",
    "    azure_endpoint=\"https://neoaihackathon.services.ai.azure.com/models/chat/completions?api-version=2024-05-01-preview\" \n",
    ")\n",
    "\n",
    "# Simple call\n",
    "response = llm.invoke(\"Hi\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14e69904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I‚Äôm quite familiar with the **contacts‚ÄØ&‚ÄØleads** space‚Äîboth the high‚Äëlevel concepts that drive sales‚Äë and marketing‚Äëfocused organizations and the day‚Äëto‚Äëday operational details that keep those processes running smoothly. Here‚Äôs a quick snapshot of the areas I‚Äôm comfortable with:\n",
      "\n",
      "| Domain | What I Know / Can Help With |\n",
      "|--------|-----------------------------|\n",
      "| **Lead Generation** | ‚Ä¢ Inbound (content marketing, SEO, social, webinars, referrals) <br>‚Ä¢ Outbound (cold email, cold calling, LinkedIn outreach, account‚Äëbased prospecting) <br>‚Ä¢ Paid acquisition (PPC, display, retargeting, lead‚Äëgen forms) |\n",
      "| **Lead Capture & Enrichment** | ‚Ä¢ Form design, progressive profiling, hidden fields, UTM tracking <br>‚Ä¢ Enrichment via APIs (Clearbit, ZoomInfo, LinkedIn, Apollo) <br>‚Ä¢ Validation (email verification, phone formatting) |\n",
      "| **Lead Scoring & Qualification** | ‚Ä¢ Rule‚Äëbased scoring (demographic, firmographic, behavioral) <br>‚Ä¢ Predictive/AI scoring (machine‚Äëlearning models, propensity to convert) <br>‚Ä¢ Qualification frameworks (BANT, CHAMP, MEDDIC, ANUM) |\n",
      "| **Lead Nurturing & Automation** | ‚Ä¢ Drip email sequences, SMS, retargeting ads <br>‚Ä¢ Marketing automation platforms (HubSpot, Marketo, Pardot, ActiveCampaign) <br>‚Ä¢ Triggered workflows, lead‚Äëstage transitions, lead‚Äëto‚Äëopportunity handoff |\n",
      "| **Contact Management** | ‚Ä¢ Centralized CRM data model (accounts, contacts, opportunities) <br>‚Ä¢ Data hygiene (deduplication, normalization, GDPR/CCPA compliance) <br>‚Ä¢ Segmentation, list building, dynamic lists |\n",
      "| **Sales‚ÄëMarketing Alignment** | ‚Ä¢ SLA definitions (lead response time, MQL‚ÜíSQL conversion) <br>‚Ä¢ Lead routing (round‚Äërobin, territory, AI‚Äëbased) <br>‚Ä¢ Reporting & attribution (first‚Äëtouch, multi‚Äëtouch, revenue‚Äëattributed) |\n",
      "| **Analytics & Reporting** | ‚Ä¢ Funnel metrics (conversion rates, velocity, win‚Äërate) <br>‚Ä¢ Attribution models (linear, time‚Äëdecay, position‚Äëbased) <br>‚Ä¢ Dashboard tools (Looker, Tableau, Power‚ÄØBI, native CRM dashboards) |\n",
      "| **Compliance & Governance** | ‚Ä¢ GDPR, CCPA, CAN‚ÄëSPAM, TCPA considerations <br>‚Ä¢ Consent capture, preference centers, data‚Äësubject requests <br>‚Ä¢ Auditing and data‚Äëretention policies |\n",
      "| **Technology Stack** | ‚Ä¢ CRMs (Salesforce, HubSpot, Dynamics 365, Pipedrive) <br>‚Ä¢ Marketing automation (Marketo, Eloqua, Mailchimp, Klaviyo) <br>‚Ä¢ Integration platforms (Zapier, Workato, MuleSoft, Tray.io) <br>‚Ä¢ Data quality tools (RingLead, Dedupely, OpenRefine) |\n",
      "| **Best‚ÄëPractice Processes** | ‚Ä¢ Lead lifecycle design (capture ‚Üí nurture ‚Üí qualify ‚Üí convert) <br>‚Ä¢ Contact enrichment & segmentation strategy <br>‚Ä¢ Continuous improvement loops (A/B testing, feedback loops, data‚Äëdriven optimization) |\n",
      "\n",
      "### How I Can Assist You\n",
      "- **Strategy**: Build or refine a lead‚Äëto‚Äërevenue framework tailored to your industry and buyer journey.  \n",
      "- **Tool Selection & Integration**: Compare platforms, design data flows, and set up connectors (e.g., Salesforce ‚Üî HubSpot ‚Üî ZoomInfo).  \n",
      "- **Implementation Guidance**: Step‚Äëby‚Äëstep playbooks for form creation, scoring models, automation workflows, and handoff processes.  \n",
      "- **Data Hygiene & Governance**: Set up deduplication rules, consent management, and compliance checklists.  \n",
      "- **Analytics**: Design dashboards, define KPIs, and interpret funnel metrics to surface actionable insights.  \n",
      "- **Training & Documentation**: Create SOPs, cheat‚Äësheets, and onboarding material for sales/marketing teams.\n",
      "\n",
      "If you have a specific challenge‚Äîlike building a predictive lead‚Äëscoring model, cleaning a messy contact database, or aligning sales‚Äëmarketing SLAs‚Äîjust let me know and I can dive deeper into that area.\n"
     ]
    }
   ],
   "source": [
    "# Simple call\n",
    "response = llm.invoke(\"how well do you understand the contacts and leads domain?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "71a8035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_document = \"\"\"Statement of Work (SOW)\n",
    "This Statement of Work (&quot;SOW&quot;) is entered into by and between Patrick and Fredrick\n",
    "Associates Inc. (&quot;Client&quot;) and Hari and Winston Associates LLC (&quot;Service Provider&quot; or\n",
    "‚ÄúCompany‚Äù), effective as of March 1, 2025. This SOW is governed by the terms and\n",
    "conditions outlined in the Master Services Agreement (&quot;MSA&quot;) signed between the parties.\n",
    "1. Scope of Services\n",
    "Neostats Analytics LLC agrees to provide the following services to the Client:\n",
    "ÔÇ∑ Data analytics consulting\n",
    "ÔÇ∑ Dashboard development and visualization\n",
    "ÔÇ∑ Machine Learning model implementation\n",
    "ÔÇ∑ Data pipeline optimization\n",
    "ÔÇ∑ Ongoing technical support and system maintenance\n",
    "2. Deliverables\n",
    "The Service Provider will deliver the following:\n",
    "ÔÇ∑ Custom Power BI dashboards tailored to Client‚Äôs specific needs\n",
    "ÔÇ∑ Monthly reports detailing system performance and insights\n",
    "ÔÇ∑ Technical documentation and user guides\n",
    "ÔÇ∑ Regular updates on ML model improvements\n",
    "3. Project Timeline\n",
    "ÔÇ∑ Project Kickoff: March 5, 2025\n",
    "ÔÇ∑ Milestone 1: Initial assessment and requirements gathering (March 15, 2025)\n",
    "ÔÇ∑ Milestone 2: Prototype delivery (April 10, 2025)\n",
    "ÔÇ∑ Milestone 3: Final dashboard deployment (May 20, 2025)\n",
    "ÔÇ∑ Completion Date: June 15, 2025 (Subject to extension based on mutual agreement,\n",
    "with priority to Neostats Analytics LLC&#39;s scheduling constraints)\n",
    "4. Fees and Payment Terms\n",
    "The client will pay the service provider the following amount:\n",
    "ÔÇ∑ Total Fee: $150,000 USD\n",
    "ÔÇ∑ Payment Schedule:\n",
    "o $75,000 USD upon signing this SOW\n",
    "o $45,000 USD upon delivery of the prototype\n",
    "o $30,000 USD upon final acceptance\n",
    "ÔÇ∑ Late payments beyond 15 days from the due date will incur a late fee of 5% per\n",
    "month.\n",
    "\n",
    "5. Intellectual Property Rights\n",
    "All deliverables, including but not limited to software, models, and documentation, shall\n",
    "remain the exclusive property of Neostats Analytics LLC unless otherwise agreed in writing.\n",
    "6. Confidentiality\n",
    "Both parties agree to maintain the confidentiality of proprietary information. However,\n",
    "Neostats Analytics LLC reserves the right to use anonymized data insights for future\n",
    "research and development purposes.\n",
    "7. Termination Clause\n",
    "ÔÇ∑ The Client may terminate this agreement with 60 days&#39; written notice.\n",
    "ÔÇ∑ Neostats Analytics LLC reserves the right to terminate with 30 days&#39; written notice,\n",
    "with all outstanding payments due immediately upon termination.\n",
    "8. Limitation of Liability\n",
    "The total liability of Neostats Analytics LLC under this agreement shall not exceed $150,000\n",
    "USD. Neostats shall not be liable for indirect, incidental, or consequential damages.\n",
    "9. Force Majeure\n",
    "The Service Provider shall not be excused from the performance of any obligations under\n",
    "this Agreement on account of Force Majeure events. Notwithstanding the occurrence of any\n",
    "event beyond the control of the Service Provider, including but not limited to acts of God,\n",
    "governmental restrictions, strikes, lockouts, riots, epidemics, or natural disasters, the Service\n",
    "Provider shall remain fully liable for timely and complete performance of all its obligations\n",
    "under this Agreement. The Client shall, however, be excused from performance to the extent\n",
    "affected by Force Majeure events.\n",
    "10. Indemnity\n",
    "The Company agrees to indemnify and hold harmless the Client, its directors, officers, and\n",
    "employees against any third-party claims, damages, or expenses (including reasonable legal\n",
    "fees) directly arising out of (i) a material breach of this Agreement by the Company, or (ii)\n",
    "willful misconduct or gross negligence of the Company or its personnel in the performance of\n",
    "obligations under this Agreement. The Company‚Äôs liability under this indemnity shall,\n",
    "however, not extend to any claims resulting from (a) the Client‚Äôs negligence, misconduct, or\n",
    "breach of this Agreement, or (b) indirect, consequential, or punitive damages, except to the\n",
    "extent such exclusion is prohibited by law.\n",
    "\n",
    "11. Dispute Resolution Mechanism\n",
    "Any dispute, controversy, or claim arising out of or relating to this Agreement, or the breach,\n",
    "termination, or validity thereof, shall be resolved exclusively in the state or federal courts\n",
    "located in New York, United States. Each party irrevocably submits to the personal\n",
    "jurisdiction of such courts and waives any objection based on forum non conveniens or any\n",
    "other objection to venue.\n",
    "This Agreement shall be governed by and construed in accordance with the laws of the\n",
    "State of New York, without regard to its conflict of law provisions.\n",
    "\n",
    "In the event of any action or proceeding to enforce rights under this Agreement, the\n",
    "prevailing party shall be entitled to recover its reasonable attorneys‚Äô fees and costs.\n",
    "12. Amendments\n",
    "Any amendments to this SOW must be made in writing and signed by authorized\n",
    "representatives of both parties. Neostats Analytics LLC reserves the right to adjust the\n",
    "project scope and timelines due to unforeseen technical complexities, with prior notice.\n",
    "13. Signatures\n",
    "For Patrick and Fredrick Associates Inc.\n",
    "Name: ___________________________\n",
    "Title: ____________________________\n",
    "Date: ____________________________\n",
    "For Hari and Winston Associates LLC\n",
    "Name: ___________________________\n",
    "Title: ____________________________\n",
    "Date: ____________________________\"\"\"\n",
    "\n",
    "# response = llm.invoke(f\"Summarize the following contract document: {contract_document}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69a0981e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='**Statement of Work (SOW) ‚Äì Summary**\\n\\n| Section | Key Points |\\n|---------|------------|\\n| **Parties & Effective Date** | ‚Ä¢ **Client:** Patrick and Fredrick Associates Inc.<br>‚Ä¢ **Service Provider (Company):** Hari and Winston Associates LLC (operating as Neostats Analytics LLC).<br>‚Ä¢ Effective **March\\u202f1\\u202f2025**; governed by the Master Services Agreement (MSA). |\\n| **Scope of Services** | ‚Ä¢ Data‚Äëanalytics consulting<br>‚Ä¢ Dashboard development & visualization (Power\\u202fBI)<br>‚Ä¢ Machine‚Äëlearning (ML) model implementation<br>‚Ä¢ Data‚Äëpipeline optimization<br>‚Ä¢ Ongoing technical support & system maintenance |\\n| **Deliverables** | ‚Ä¢ Custom Power\\u202fBI dashboards<br>‚Ä¢ Monthly performance/insight reports<br>‚Ä¢ Technical documentation & user guides<br>‚Ä¢ Regular updates on ML model improvements |\\n| **Project Timeline** | ‚Ä¢ **Kick‚Äëoff:** March\\u202f5\\u202f2025<br>‚Ä¢ **Milestone\\u202f1 ‚Äì Assessment/Requirements:** March\\u202f15\\u202f2025<br>‚Ä¢ **Milestone\\u202f2 ‚Äì Prototype:** April\\u202f10\\u202f2025<br>‚Ä¢ **Milestone\\u202f3 ‚Äì Final Dashboard:** May\\u202f20\\u202f2025<br>‚Ä¢ **Completion (subject to extension):** June\\u202f15\\u202f2025 |\\n| **Fees & Payment Terms** | ‚Ä¢ **Total fee:** **$150,000 USD**<br>‚Ä¢ **Payment schedule:**<br>\\u2003‚Äì $75,000 on signing<br>\\u2003‚Äì $45,000 on prototype delivery<br>\\u2003‚Äì $30,000 on final acceptance<br>‚Ä¢ Late payments (>15\\u202fdays) incur **5\\u202f% per month** interest |\\n| **Intellectual Property (IP)** | All deliverables (software, models, docs, etc.) remain the **exclusive property of Neostats Analytics LLC**, unless a written amendment states otherwise. |\\n| **Confidentiality** | Both parties must keep proprietary information confidential. Neostats may use **anonymized data insights** for its own R&D. |\\n| **Termination** | ‚Ä¢ Client may terminate with **60\\u202fdays written notice**.<br>‚Ä¢ Service Provider may terminate with **30\\u202fdays written notice**; all outstanding fees become immediately due. |\\n| **Limitation of Liability** | Provider‚Äôs total liability capped at **$150,000** (the contract price). No liability for indirect, incidental, or consequential damages. |\\n| **Force Majeure** | ‚Ä¢ Provider **cannot** invoke force‚Äëmajeure to avoid performance; remains fully liable for timely delivery.<br>‚Ä¢ Client is excused only to the extent its performance is affected by such events. |\\n| **Indemnity** | Provider indemnifies Client (and its officers, directors, employees) against third‚Äëparty claims arising from: <br>\\u20031. Material breach of the SOW, or <br>\\u20032. Willful misconduct/gross negligence by Provider or its personnel.<br>Exclusions: claims caused by Client‚Äôs own negligence or indirect/punitive damages (unless prohibited by law). |\\n| **Dispute Resolution & Governing Law** | ‚Ä¢ All disputes resolved exclusively in **state or federal courts in New York, USA**.<br>‚Ä¢ Governing law: **State of New York** (no conflict‚Äëof‚Äëlaw rules).<br>‚Ä¢ Prevailing party may recover reasonable attorney‚Äôs fees. |\\n| **Amendments** | Any changes must be **written and signed** by authorized representatives of both parties. Provider may adjust scope/timelines for unforeseen technical issues with prior notice. |\\n| **Signatures** | Spaces provided for authorized signatories of both parties (name, title, date). |\\n\\n**Bottom‚ÄëLine Takeaways**\\n\\n- **What‚Äôs being delivered?** Custom Power\\u202fBI dashboards, monthly performance reports, documentation, and ongoing ML/model support.\\n- **When?** Kick‚Äëoff March\\u202f5\\u202f2025; final delivery by May\\u202f20\\u202f2025; overall completion by June\\u202f15\\u202f2025 (extendable by mutual agreement).\\n- **Cost?** $150\\u202fk total, paid in three installments; late fees apply after 15 days.\\n- **Who owns the work?** Neostats Analytics LLC retains all IP rights.\\n- **Risk limits:** Liability capped at contract price; provider bears most performance risk (cannot rely on force‚Äëmajeure), while client can exit with 60‚Äëday notice.\\n- **Legal venue:** New York courts; prevailing party gets attorney fees. \\n\\nThis summary captures the essential obligations, timelines, financial terms, and legal protections of the SOW.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 1026, 'prompt_tokens': 1172, 'total_tokens': 2198, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'audio_prompt_tokens': 0, 'reasoning_tokens': 0}, 'model_name': 'gpt-oss-120b', 'system_fingerprint': None, 'id': '14c07ffb79d14fe7a15f58edfdef5dc3', 'service_tier': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run--32fd9076-bb12-4fb5-886c-c4ff11e19c4c-0' usage_metadata={'input_tokens': 1172, 'output_tokens': 1026, 'total_tokens': 2198, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53666b21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce96b980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "\n",
    "# Set your Azure OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"Azure_Open_API\"\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    deployment_name=\"neostats_hackathon_api_v1\",  # Your embedding deployment\n",
    "    model=\"text-embedding-3-large\",\n",
    "    chunk_size=1,\n",
    "    openai_api_version=\"2024-05-01-preview\",\n",
    "    # api_key=\"Azure_Open_API\",                      # Replace with your actual key\n",
    "    azure_endpoint=\"https://neoaihackathon.services.ai.azure.com/\"\n",
    ")\n",
    "\n",
    "vector = embeddings.embed_query(\"Hello world\")\n",
    "print(vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58202297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3566/181629573.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "2025-09-27 11:15:39.860524: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03447728976607323, 0.03102320060133934, 0.006734994240105152, 0.026108931750059128, -0.039361972361803055, -0.16030248999595642, 0.06692398339509964, -0.006441479083150625, -0.04745054244995117, 0.014758830890059471, 0.07087532430887222, 0.05552747845649719, 0.019193345680832863, -0.026251330971717834, -0.010109461843967438, -0.026940541341900826, 0.022307412698864937, -0.022226640954613686, -0.1496926248073578, -0.017493126913905144, 0.007676230277866125, 0.054352302104234695, 0.0032544878777116537, 0.03172598034143448, -0.08462141454219818, -0.02940593659877777, 0.05159566178917885, 0.04812410846352577, -0.003314803820103407, -0.05827922001481056, 0.04196928068995476, 0.02221076376736164, 0.128188818693161, -0.022338951006531715, -0.011656216345727444, 0.06292843818664551, -0.03287629410624504, -0.0912260115146637, -0.031175369396805763, 0.05269953981041908, 0.047034818679094315, -0.08420302718877792, -0.03005615621805191, -0.02074483223259449, 0.009517730213701725, -0.0037217524368315935, 0.007343302480876446, 0.03932436183094978, 0.09327400475740433, -0.003788554575294256, -0.05274209752678871, -0.058058224618434906, -0.006864419672638178, 0.005283195525407791, 0.08289302885532379, 0.019362695515155792, 0.00628450233489275, -0.010330773890018463, 0.00903241615742445, -0.03768377751111984, -0.045206110924482346, 0.024016324430704117, -0.006944168824702501, 0.013491644524037838, 0.10005495697259903, -0.07168382406234741, -0.02169504202902317, 0.03161852806806564, -0.05163465067744255, -0.0822477862238884, -0.06569333374500275, -0.009895375929772854, 0.005816405639052391, 0.07355455309152603, -0.034050315618515015, 0.02488611824810505, 0.014488087967038155, 0.026457348838448524, 0.009656681679189205, 0.030217377468943596, 0.052804019302129745, -0.07535985112190247, 0.00989717897027731, 0.02983681857585907, 0.0175556018948555, 0.02309194579720497, 0.0019338352140039206, 0.0014001672388985753, -0.04717596620321274, -0.01119436975568533, -0.1142013743519783, -0.019812043756246567, 0.04026622697710991, 0.0021930362563580275, -0.07979222387075424, -0.02538231387734413, 0.09448296576738358, -0.02898113802075386, -0.14500245451927185, 0.23097743093967438, 0.0277311522513628, 0.03211146965622902, 0.031065024435520172, 0.04283282160758972, 0.06423783302307129, 0.03216314688324928, -0.004876771476119757, 0.05569949373602867, -0.03753240406513214, -0.021505609154701233, -0.028342748060822487, -0.028846891596913338, 0.038353051990270615, -0.017468681558966637, 0.05248531699180603, -0.07487605512142181, -0.031259678304195404, 0.021841570734977722, -0.03989563509821892, -0.008587094023823738, 0.02695666253566742, -0.04849553108215332, 0.011469907127320766, 0.02961827628314495, -0.020572196692228317, 0.013103894889354706, 0.02883346565067768, -3.194200554190157e-33, 0.06478207558393478, -0.01813019998371601, 0.05178993567824364, 0.12198273092508316, 0.028780221939086914, 0.008722052909433842, -0.07052110135555267, -0.016907233744859695, 0.04073972627520561, 0.042116206139326096, 0.025447217747569084, 0.035746246576309204, -0.04914472997188568, 0.0021290620788931847, -0.015546487644314766, 0.0507306270301342, -0.0481853187084198, 0.03588063642382622, -0.0040670656599104404, 0.1017247885465622, -0.055970024317502975, -0.010681015439331532, 0.011235799640417099, 0.09068648517131805, 0.004234458785504103, 0.03513864055275917, -0.009702797047793865, -0.09386519342660904, 0.09285557270050049, 0.008004930801689625, -0.007705324329435825, -0.05208674073219299, -0.012588007375597954, 0.0032668921630829573, 0.006013547535985708, 0.007581594865769148, 0.010517173446714878, -0.08634552359580994, -0.06987878680229187, -0.002533871680498123, -0.09097658097743988, 0.046887338161468506, 0.052076440304517746, 0.007193905767053366, 0.01090361550450325, -0.005229537840932608, 0.01393729168921709, 0.021968310698866844, 0.034208543598651886, 0.06022472679615021, 0.00011663127224892378, 0.014731918461620808, -0.07008922845125198, 0.028499038890004158, -0.027601659297943115, 0.010768412612378597, 0.034830961376428604, -0.022487886250019073, 0.009769070893526077, 0.07722782343626022, 0.021588345989584923, 0.11495617777109146, -0.06800113618373871, 0.02376101166009903, -0.015983890742063522, -0.017827006056904793, 0.06439490616321564, 0.0320257693529129, 0.05027024820446968, -0.005913702305406332, -0.033708006143569946, 0.01784030720591545, 0.016573401167988777, 0.06329657137393951, 0.0346771664917469, 0.0464734323322773, 0.09790615737438202, -0.006635445170104504, 0.02520700916647911, -0.07798835635185242, 0.016926439478993416, -0.0009457569685764611, 0.022471873089671135, -0.03825318440794945, 0.09570477902889252, -0.005350756458938122, 0.01046903058886528, -0.11524057388305664, -0.013262517750263214, -0.010709383524954319, -0.08311723917722702, 0.07327354699373245, 0.049392230808734894, -0.008994434960186481, -0.0958455502986908, 3.366148929092564e-33, 0.12493187189102173, 0.01934971660375595, -0.05822568014264107, -0.03598824143409729, -0.0507468581199646, -0.045662350952625275, -0.08260340988636017, 0.1481947898864746, -0.08842120319604874, 0.060274481773376465, 0.05103021487593651, 0.010303177870810032, 0.14121422171592712, 0.030813852325081825, 0.06103307381272316, -0.05285125598311424, 0.13664889335632324, 0.00918989535421133, -0.017325229942798615, -0.012848606333136559, -0.00799524039030075, -0.050980061292648315, -0.05235060676932335, 0.007593069691210985, -0.015166294761002064, 0.016960347071290016, 0.021270563825964928, 0.02055799961090088, -0.1200280413031578, 0.014461862854659557, 0.026759888976812363, 0.025330614298582077, -0.042754676192998886, 0.00676845433190465, -0.01445860881358385, 0.045261967927217484, -0.09147657454013824, -0.01943913660943508, -0.01783348061144352, -0.05491006374359131, -0.05264110863208771, -0.010459003038704395, -0.05201605334877968, 0.020892063155770302, -0.07997031509876251, -0.01211128756403923, -0.05773143842816353, 0.02317827008664608, -0.008031640201807022, -0.025989236310124397, -0.07995668798685074, -0.020728858187794685, 0.0488177128136158, -0.02038922719657421, -0.049176592379808426, 0.014159638434648514, -0.06362210214138031, -0.007807409390807152, 0.016431493684649467, -0.02568250149488449, 0.013381117954850197, 0.02624870277941227, 0.009978334419429302, 0.06322893500328064, 0.0026720359455794096, -0.006582849659025669, 0.01663200370967388, 0.03236650303006172, 0.03794250637292862, -0.036376047879457474, -0.006910975556820631, 0.00015967067156452686, -0.0016335599357262254, -0.027278179302811623, -0.02803799882531166, 0.04968149960041046, -0.02886713109910488, -0.0024180489126592875, 0.014774887822568417, 0.009764575399458408, 0.005797557532787323, 0.013486091047525406, 0.0055678957141935825, 0.03722700476646423, 0.007232528645545244, 0.040156275033950806, 0.08150320500135422, 0.07199167460203171, -0.013056188821792603, -0.042882103472948074, -0.011011225171387196, 0.004897794220596552, -0.009229747578501701, 0.03519142419099808, -0.051035039126873016, -1.571437557856825e-08, -0.08862444758415222, 0.02390935830771923, -0.016238726675510406, 0.03170045092701912, 0.027284150943160057, 0.05246875435113907, -0.04707089811563492, -0.058847520500421524, -0.0632082149386406, 0.04088851436972618, 0.049827948212623596, 0.10655166208744049, -0.07450231164693832, -0.012495413422584534, 0.018370646983385086, 0.03947410359978676, -0.024797923862934113, 0.014516261406242847, -0.037069179117679596, 0.020015783607959747, -4.8564219468971714e-05, 0.009866546839475632, 0.024838782846927643, -0.05245813727378845, 0.029314156621694565, -0.08719193935394287, -0.01449981052428484, 0.026019036769866943, -0.01874639093875885, -0.07620516419410706, 0.03504329174757004, 0.10363951325416565, -0.028050502762198448, 0.012718225829303265, -0.07632554322481155, -0.01865241304039955, 0.024976739659905434, 0.08144533634185791, 0.06875890493392944, -0.06405659765005112, -0.08389391005039215, 0.061362411826848984, -0.033545561134815216, -0.10615338385105133, -0.04008056968450546, 0.03253021836280823, 0.07662488520145416, -0.07301618903875351, 0.0003375604283064604, -0.04087160527706146, -0.07578858733177185, 0.027527663856744766, 0.07462544739246368, 0.01771727204322815, 0.09121841937303543, 0.11022017896175385, 0.0005697802407667041, 0.05146327242255211, -0.014551307074725628, 0.03323200345039368, 0.023792194202542305, -0.02288983389735222, 0.038937486708164215, 0.03020675852894783]\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Load all-MiniLM model\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Embed a sentence\n",
    "vector = embeddings.embed_query(\"Hello world\")\n",
    "print(vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec062da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87207858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc length: 56000\n",
      "Chunk size: 2000, Overlap: 400, Total chunks: 35\n",
      "Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet. Lore\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Load all-MiniLM model\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "def dynamic_chunker(text: str, C_min=400, C_max=2000, alpha=30, beta=0.20):\n",
    "    N = len(text)\n",
    "\n",
    "    # Formula-based chunk size\n",
    "    chunk_size = min(C_max, max(C_min, int(alpha * (N ** 0.6))))\n",
    "    overlap = int(beta * chunk_size)\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    chunks = splitter.split_text(text)\n",
    "    return chunks, chunk_size, overlap\n",
    "\n",
    "# Example\n",
    "doc = \"Lorem ipsum dolor sit amet. \" * 2000\n",
    "chunks, size, overlap = dynamic_chunker(doc)\n",
    "\n",
    "print(f\"Doc length: {len(doc)}\")\n",
    "print(f\"Chunk size: {size}, Overlap: {overlap}, Total chunks: {len(chunks)}\")\n",
    "print(chunks[0][:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8bfa9866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "\n",
    "# Example contract text\n",
    "doc = \"\"\"\n",
    "1. DEFINITIONS\n",
    "This Agreement defines \"Confidential Information\" as any business, technical, or financial information disclosed.\n",
    "\n",
    "2. OBLIGATIONS OF THE DEVELOPER\n",
    "The Developer shall deliver the software product within 90 days of execution.\n",
    "\n",
    "3. PAYMENT TERMS\n",
    "The Client shall pay $50,000 within 30 days of delivery.\n",
    "\n",
    "ARTICLE IV TERMINATION\n",
    "Either party may terminate this Agreement upon material breach.\n",
    "\"\"\"\n",
    "\n",
    "  # section content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c734972",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_nlp = nlp(contract_document)\n",
    "\n",
    "for sent in doc_nlp.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c2d0956d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ARTICLE IV TERMINATION ---\n",
      "Sentences:\n",
      "   Either party may terminate this Agreement upon material breach.\n",
      "Entities: [{'text': 'Agreement', 'label': 'PRODUCT'}]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def parse_contract_with_ner(text: str):\n",
    "    \"\"\"\n",
    "    Parses a contract into sections with sentences and named entities.\n",
    "    Returns a dict: {section_title: {\"sentences\": [], \"entities\": []}}\n",
    "    \"\"\"\n",
    "    # Regex for common heading patterns (numbered, ARTICLE, ALL-CAPS)\n",
    "    section_pattern = re.compile(\n",
    "        r\"(?P<header>(?:^|\\n)(?:\\d+(\\.\\d+)*\\s+.+?|ARTICLE\\s+[IVXLC]+\\s*.*|[A-Z][A-Z\\s\\-]{3,}))\",\n",
    "        re.MULTILINE\n",
    "    )\n",
    "    \n",
    "    matches = list(section_pattern.finditer(text))\n",
    "    sections_dict = {}\n",
    "\n",
    "    for i, match in enumerate(matches):\n",
    "        start = match.end()\n",
    "        end = matches[i+1].start() if i+1 < len(matches) else len(text)\n",
    "\n",
    "        header = match.group(\"header\").strip()\n",
    "        content = text[start:end].strip()\n",
    "\n",
    "        # Process content with spaCy\n",
    "        doc_nlp = nlp(content)\n",
    "        sentences = [sent.text.strip() for sent in doc_nlp.sents]\n",
    "\n",
    "        # Extract named entities (PERSON, ORG, DATE, MONEY, GPE, etc.)\n",
    "        entities = [{\"text\": ent.text, \"label\": ent.label_} for ent in doc_nlp.ents]\n",
    "\n",
    "        sections_dict[header] = {\n",
    "            \"sentences\": sentences,\n",
    "            \"entities\": entities\n",
    "        }\n",
    "\n",
    "    return sections_dict\n",
    "\n",
    "# ------------------ Example Usage ------------------\n",
    "contract_document = \"\"\"\n",
    "1. DEFINITIONS\n",
    "This Agreement defines \"Confidential Information\" as any business, technical, or financial information disclosed by Acme Ltd.\n",
    "\n",
    "2. OBLIGATIONS OF THE DEVELOPER\n",
    "The Developer shall deliver the software product within 90 days of execution. The Client, Beta Corp, will provide access to all required resources.\n",
    "\n",
    "3. PAYMENT TERMS\n",
    "The Client shall pay $50,000 within 30 days of delivery.\n",
    "\n",
    "ARTICLE IV TERMINATION\n",
    "Either party may terminate this Agreement upon material breach.\n",
    "\"\"\"\n",
    "\n",
    "sections = parse_contract_with_ner(contract_document)\n",
    "\n",
    "# Preview the parsed sections\n",
    "for title, data in sections.items():\n",
    "    print(f\"--- {title} ---\")\n",
    "    print(\"Sentences:\")\n",
    "    for s in data[\"sentences\"]:\n",
    "        print(\"  \", s)\n",
    "    print(\"Entities:\", data[\"entities\"])\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b64f47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PARSING CONTRACT ===\n",
      "\n",
      "--- FULL_DOCUMENT ---\n",
      "Content Length: 5221 characters\n",
      "Word Count: 771\n",
      "Sentences: 50\n",
      "\n",
      "Key Entities:\n",
      "  Parties: ['Patrick', 'Fredrick Associates Inc.', 'Hari', 'Winston Associates LLC (&quot;Service Provider&quot', 'the Master Services Agreement (&quot;MSA&quot', 'Dashboard', 'Machine Learning', 'Deliverables The Service Provider', 'Custom Power BI', 'ML', 'Project Timeline \\uf0b7', 'Neostats Analytics', 'USD', 'Neostats Analytics', 'Neostats Analytics', 'Force Majeure', 'the Service Provider', 'Force Majeure', 'Company', 'the State of New York', 'Patrick', 'Fredrick Associates Inc. Name', 'Hari', 'Winston Associates LLC Name']\n",
      "  Locations: ['Client', 'Client', 'New York', 'United States']\n",
      "  Dates: ['March 1, 2025', 'Monthly', 'March 5, 2025', 'March 15, 2025', 'April 10, 2025', 'May 20, 2025', 'June 15, 2025', '15 days']\n",
      "  Money: ['$150,000 USD', '$75,000', '$45,000', '$30,000 USD', '$150,000 USD']\n",
      "  Other: ['1.', '2', '3', 'Milestone 1', '4', 'USD', '5%', '5', '6', '60 days&#39', '30', '8.', '9', 'Agreement', 'Agreement', '10', 'third', 'Agreement', '11', 'Agreement', '12', '13']\n",
      "\n",
      "First 2 sentences:\n",
      "  1. Statement of Work (SOW)\n",
      "  2. This Statement of Work (&quot;SOW&quot;) is entered into by and between Patrick and Fredrick Associates Inc. (&quot;Client&quot;) and Hari and Winston Associates LLC (&quot;Service Provider&quot; or ‚ÄúCompany‚Äù), effective as of March 1, 2025.\n",
      "\n",
      "Clean tokens (first 10): ['statement', 'work', 'sow', 'statement', 'work', 'quot;sow&quot', 'enter', 'patrick', 'fredrick', 'associates']\n",
      "--------------------------------------------------\n",
      "\n",
      "=== RAG CHUNKS ===\n",
      "\n",
      "Chunk 1 (Section: FULL_DOCUMENT):\n",
      "Content: Statement of Work (SOW) This Statement of Work (&quot;SOW&quot;) is entered into by and between Patrick and Fredrick Associates Inc. (&quot;Client&quot;) and Hari and Winston Associates LLC (&quot;Ser...\n",
      "\n",
      "Chunk 2 (Section: FULL_DOCUMENT):\n",
      "Content: This SOW is governed by the terms and conditions outlined in the Master Services Agreement (&quot;MSA&quot;) signed between the parties....\n",
      "\n",
      "Chunk 3 (Section: FULL_DOCUMENT):\n",
      "Content: 1. Scope of Services Neostats Analytics LLC agrees to provide the following services to the Client: ÔÇ∑ Data analytics consulting ÔÇ∑ Dashboard development and visualization ÔÇ∑ Machine Learning model imple...\n",
      "\n",
      "=== SUMMARY STATISTICS ===\n",
      "      Section  Word Count  Sentence Count  Parties  Dates  Money  Locations\n",
      "FULL_DOCUMENT         771              50       24      8      5          4\n",
      "\n",
      "=== ALL ENTITIES EXTRACTED ===\n",
      "      Section      Type                                                Text    Label\n",
      "FULL_DOCUMENT   parties                                             Patrick      ORG\n",
      "FULL_DOCUMENT   parties                            Fredrick Associates Inc.      ORG\n",
      "FULL_DOCUMENT   parties                                                Hari      ORG\n",
      "FULL_DOCUMENT   parties Winston Associates LLC (&quot;Service Provider&quot      ORG\n",
      "FULL_DOCUMENT   parties       the Master Services Agreement (&quot;MSA&quot      ORG\n",
      "FULL_DOCUMENT   parties                                           Dashboard   PERSON\n",
      "FULL_DOCUMENT   parties                                    Machine Learning   PERSON\n",
      "FULL_DOCUMENT   parties                   Deliverables The Service Provider      ORG\n",
      "FULL_DOCUMENT   parties                                     Custom Power BI      ORG\n",
      "FULL_DOCUMENT   parties                                                  ML      ORG\n",
      "FULL_DOCUMENT   parties                                  Project Timeline ÔÇ∑      ORG\n",
      "FULL_DOCUMENT   parties                                  Neostats Analytics      ORG\n",
      "FULL_DOCUMENT   parties                                                 USD      ORG\n",
      "FULL_DOCUMENT   parties                                  Neostats Analytics      ORG\n",
      "FULL_DOCUMENT   parties                                  Neostats Analytics      ORG\n",
      "FULL_DOCUMENT   parties                                       Force Majeure      ORG\n",
      "FULL_DOCUMENT   parties                                the Service Provider      ORG\n",
      "FULL_DOCUMENT   parties                                       Force Majeure      ORG\n",
      "FULL_DOCUMENT   parties                                             Company      ORG\n",
      "FULL_DOCUMENT   parties                               the State of New York      ORG\n",
      "FULL_DOCUMENT   parties                                             Patrick   PERSON\n",
      "FULL_DOCUMENT   parties                       Fredrick Associates Inc. Name      ORG\n",
      "FULL_DOCUMENT   parties                                                Hari      ORG\n",
      "FULL_DOCUMENT   parties                         Winston Associates LLC Name      ORG\n",
      "FULL_DOCUMENT locations                                              Client      GPE\n",
      "FULL_DOCUMENT locations                                              Client      GPE\n",
      "FULL_DOCUMENT locations                                            New York      GPE\n",
      "FULL_DOCUMENT locations                                       United States      GPE\n",
      "FULL_DOCUMENT     dates                                       March 1, 2025     DATE\n",
      "FULL_DOCUMENT     dates                                             Monthly     DATE\n",
      "FULL_DOCUMENT     dates                                       March 5, 2025     DATE\n",
      "FULL_DOCUMENT     dates                                      March 15, 2025     DATE\n",
      "FULL_DOCUMENT     dates                                      April 10, 2025     DATE\n",
      "FULL_DOCUMENT     dates                                        May 20, 2025     DATE\n",
      "FULL_DOCUMENT     dates                                       June 15, 2025     DATE\n",
      "FULL_DOCUMENT     dates                                             15 days     DATE\n",
      "FULL_DOCUMENT     money                                        $150,000 USD    MONEY\n",
      "FULL_DOCUMENT     money                                             $75,000    MONEY\n",
      "FULL_DOCUMENT     money                                             $45,000    MONEY\n",
      "FULL_DOCUMENT     money                                         $30,000 USD    MONEY\n",
      "FULL_DOCUMENT     money                                        $150,000 USD    MONEY\n",
      "FULL_DOCUMENT     other                                                  1. CARDINAL\n",
      "FULL_DOCUMENT     other                                                   2 CARDINAL\n",
      "FULL_DOCUMENT     other                                                   3 CARDINAL\n",
      "FULL_DOCUMENT     other                                         Milestone 1  PRODUCT\n",
      "FULL_DOCUMENT     other                                                   4 CARDINAL\n",
      "FULL_DOCUMENT     other                                                 USD  PRODUCT\n",
      "FULL_DOCUMENT     other                                                  5%  PERCENT\n",
      "FULL_DOCUMENT     other                                                   5 CARDINAL\n",
      "FULL_DOCUMENT     other                                                   6 CARDINAL\n",
      "FULL_DOCUMENT     other                                         60 days&#39 QUANTITY\n",
      "FULL_DOCUMENT     other                                                  30 CARDINAL\n",
      "FULL_DOCUMENT     other                                                  8. CARDINAL\n",
      "FULL_DOCUMENT     other                                                   9 CARDINAL\n",
      "FULL_DOCUMENT     other                                           Agreement  PRODUCT\n",
      "FULL_DOCUMENT     other                                           Agreement  PRODUCT\n",
      "FULL_DOCUMENT     other                                                  10 CARDINAL\n",
      "FULL_DOCUMENT     other                                               third  ORDINAL\n",
      "FULL_DOCUMENT     other                                           Agreement  PRODUCT\n",
      "FULL_DOCUMENT     other                                                  11 CARDINAL\n",
      "FULL_DOCUMENT     other                                           Agreement  PRODUCT\n",
      "FULL_DOCUMENT     other                                                  12 CARDINAL\n",
      "FULL_DOCUMENT     other                                                  13 CARDINAL\n",
      "\n",
      "=== PROCESSING COMPLETE ===\n",
      "Total sections processed: 1\n",
      "Total chunks created: 22\n",
      "Total entities extracted: 63\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "import string\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import unicodedata\n",
    "\n",
    "# Load spaCy model with custom configuration\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class ContractParser:\n",
    "    \"\"\"\n",
    "    Enhanced contract parser with comprehensive preprocessing for RAG systems.\n",
    "    Handles section detection, text cleaning, and NLP preprocessing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Enhanced section patterns for various legal document formats\n",
    "        self.section_patterns = [\n",
    "            # Numbered sections: \"1.\", \"1.1\", \"1.1.1\", etc.\n",
    "            r\"(?P<header>^(?:\\d+(?:\\.\\d+)*\\.?\\s+[A-Z][^\\n]*))(?=\\n)\",\n",
    "            \n",
    "            # Roman numerals: \"I.\", \"II.\", \"III.\", etc.\n",
    "            r\"(?P<header>^(?:[IVXLCDM]+\\.?\\s+[A-Z][^\\n]*))(?=\\n)\",\n",
    "            \n",
    "            # Articles: \"ARTICLE I\", \"ARTICLE 1\", etc.\n",
    "            r\"(?P<header>^(?:ARTICLE\\s+(?:[IVXLCDM]+|\\d+)[\\s\\-]*[^\\n]*))(?=\\n)\",\n",
    "            \n",
    "            # All caps headings (minimum 3 words)\n",
    "            r\"(?P<header>^(?:[A-Z][A-Z\\s\\-\\,\\&]{8,}))(?=\\n)\",\n",
    "            \n",
    "            # Section with keywords\n",
    "            r\"(?P<header>^(?:SECTION|CLAUSE|PART|CHAPTER)\\s+\\d+[\\s\\-]*[^\\n]*)(?=\\n)\",\n",
    "            \n",
    "            # Lettered sections: \"(a)\", \"(A)\", \"a)\", \"A)\", etc.\n",
    "            r\"(?P<header>^(?:\\([a-zA-Z]\\)|\\b[a-zA-Z]\\)\\s+)[A-Z][^\\n]*)(?=\\n)\",\n",
    "        ]\n",
    "        \n",
    "        # Compile all patterns\n",
    "        self.compiled_patterns = [re.compile(pattern, re.MULTILINE | re.IGNORECASE) \n",
    "                                for pattern in self.section_patterns]\n",
    "        \n",
    "        # Legal stopwords to remove during preprocessing\n",
    "        self.legal_stopwords = {\n",
    "            'shall', 'whereas', 'hereby', 'herein', 'hereof', 'hereto', 'herewith',\n",
    "            'hereunder', 'therefor', 'thereof', 'thereto', 'therein', 'whereby',\n",
    "            'aforesaid', 'aforementioned', 'said', 'such', 'same'\n",
    "        }\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text content.\"\"\"\n",
    "        # Normalize unicode characters\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        \n",
    "        # Remove excessive whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Remove page breaks and form feeds\n",
    "        text = re.sub(r'[\\f\\r\\v]', '\\n', text)\n",
    "        \n",
    "        # Standardize quotes\n",
    "        text = re.sub(r'[\"\"\"]', '\"', text)\n",
    "        text = re.sub(r\"[''']\", \"'\", text)\n",
    "        \n",
    "        # Remove excessive line breaks\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def detect_sections(self, text: str) -> List[Tuple[str, int, int]]:\n",
    "        \"\"\"\n",
    "        Detect sections in the document.\n",
    "        Returns list of tuples: (header, start_pos, end_pos)\n",
    "        \"\"\"\n",
    "        all_matches = []\n",
    "        \n",
    "        for pattern in self.compiled_patterns:\n",
    "            matches = list(pattern.finditer(text))\n",
    "            for match in matches:\n",
    "                header = match.group(\"header\").strip()\n",
    "                # Clean header\n",
    "                header = re.sub(r'\\s+', ' ', header)\n",
    "                all_matches.append((header, match.start(), match.end()))\n",
    "        \n",
    "        # Sort by position and remove duplicates\n",
    "        all_matches.sort(key=lambda x: x[1])\n",
    "        \n",
    "        # Remove overlapping matches (keep the first one)\n",
    "        filtered_matches = []\n",
    "        last_end = -1\n",
    "        for match in all_matches:\n",
    "            if match[1] >= last_end:\n",
    "                filtered_matches.append(match)\n",
    "                last_end = match[2]\n",
    "        \n",
    "        return filtered_matches\n",
    "    \n",
    "    def extract_entities_enhanced(self, doc) -> Dict[str, List]:\n",
    "        \"\"\"Extract and categorize named entities with legal context.\"\"\"\n",
    "        entities = {\n",
    "            'parties': [],      # PERSON, ORG\n",
    "            'locations': [],    # GPE, LOC\n",
    "            'dates': [],        # DATE\n",
    "            'money': [],        # MONEY\n",
    "            'laws': [],         # LAW\n",
    "            'other': []         # Miscellaneous\n",
    "        }\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            entity_info = {\n",
    "                \"text\": ent.text.strip(),\n",
    "                \"label\": ent.label_,\n",
    "                \"start\": ent.start_char,\n",
    "                \"end\": ent.end_char\n",
    "            }\n",
    "            \n",
    "            if ent.label_ in ['PERSON', 'ORG']:\n",
    "                entities['parties'].append(entity_info)\n",
    "            elif ent.label_ in ['GPE', 'LOC']:\n",
    "                entities['locations'].append(entity_info)\n",
    "            elif ent.label_ == 'DATE':\n",
    "                entities['dates'].append(entity_info)\n",
    "            elif ent.label_ == 'MONEY':\n",
    "                entities['money'].append(entity_info)\n",
    "            elif ent.label_ == 'LAW':\n",
    "                entities['laws'].append(entity_info)\n",
    "            else:\n",
    "                entities['other'].append(entity_info)\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def preprocess_for_rag(self, text: str) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        Comprehensive preprocessing for RAG system.\n",
    "        Returns tokens, lemmas, entities, and other features.\n",
    "        \"\"\"\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Extract tokens with detailed information\n",
    "        tokens = []\n",
    "        lemmas = []\n",
    "        clean_tokens = []\n",
    "        \n",
    "        for token in doc:\n",
    "            # Skip punctuation, whitespace, and stop words for clean tokens\n",
    "            if not token.is_punct and not token.is_space and not token.is_stop:\n",
    "                # Additional filter for legal documents\n",
    "                if token.lemma_.lower() not in self.legal_stopwords:\n",
    "                    clean_tokens.append(token.lemma_.lower())\n",
    "                    lemmas.append(token.lemma_)\n",
    "            \n",
    "            # Keep all tokens for reference\n",
    "            tokens.append({\n",
    "                'text': token.text,\n",
    "                'lemma': token.lemma_,\n",
    "                'pos': token.pos_,\n",
    "                'tag': token.tag_,\n",
    "                'is_alpha': token.is_alpha,\n",
    "                'is_stop': token.is_stop\n",
    "            })\n",
    "        \n",
    "        # Extract sentences\n",
    "        sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "        \n",
    "        # Extract entities\n",
    "        entities = self.extract_entities_enhanced(doc)\n",
    "        \n",
    "        # Extract noun phrases\n",
    "        noun_phrases = [chunk.text for chunk in doc.noun_chunks]\n",
    "        \n",
    "        return {\n",
    "            'tokens': tokens,\n",
    "            'lemmas': lemmas,\n",
    "            'clean_tokens': clean_tokens,\n",
    "            'sentences': sentences,\n",
    "            'entities': entities,\n",
    "            'noun_phrases': noun_phrases,\n",
    "            'word_count': len([t for t in doc if not t.is_punct and not t.is_space]),\n",
    "            'sentence_count': len(sentences)\n",
    "        }\n",
    "    \n",
    "    def parse_contract_enhanced(self, text: str) -> Dict[str, Dict]:\n",
    "        \"\"\"\n",
    "        Main parsing function with comprehensive preprocessing.\n",
    "        \"\"\"\n",
    "        # Clean the input text\n",
    "        text = self.clean_text(text)\n",
    "        \n",
    "        # Detect sections\n",
    "        section_matches = self.detect_sections(text)\n",
    "        \n",
    "        if not section_matches:\n",
    "            # If no sections found, treat entire text as one section\n",
    "            return {\n",
    "                \"FULL_DOCUMENT\": {\n",
    "                    \"content\": text,\n",
    "                    \"preprocessing\": self.preprocess_for_rag(text)\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        sections_dict = {}\n",
    "        \n",
    "        for i, (header, start, end) in enumerate(section_matches):\n",
    "            # Determine content boundaries\n",
    "            content_start = end\n",
    "            content_end = section_matches[i+1][1] if i+1 < len(section_matches) else len(text)\n",
    "            \n",
    "            # Extract content\n",
    "            content = text[content_start:content_end].strip()\n",
    "            \n",
    "            if content:  # Only process non-empty sections\n",
    "                sections_dict[header] = {\n",
    "                    \"content\": content,\n",
    "                    \"preprocessing\": self.preprocess_for_rag(content)\n",
    "                }\n",
    "        \n",
    "        return sections_dict\n",
    "    \n",
    "    def create_rag_chunks(self, sections_dict: Dict, max_chunk_size: int = 500) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Create optimized chunks for RAG system based on sentences.\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        for section_title, section_data in sections_dict.items():\n",
    "            sentences = section_data[\"preprocessing\"][\"sentences\"]\n",
    "            entities = section_data[\"preprocessing\"][\"entities\"]\n",
    "            \n",
    "            current_chunk = \"\"\n",
    "            current_entities = []\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                # Check if adding this sentence exceeds chunk size\n",
    "                if len(current_chunk) + len(sentence) > max_chunk_size and current_chunk:\n",
    "                    # Save current chunk\n",
    "                    chunks.append({\n",
    "                        \"section\": section_title,\n",
    "                        \"content\": current_chunk.strip(),\n",
    "                        \"entities\": current_entities,\n",
    "                        \"chunk_id\": len(chunks)\n",
    "                    })\n",
    "                    current_chunk = sentence\n",
    "                    current_entities = []\n",
    "                else:\n",
    "                    current_chunk += \" \" + sentence if current_chunk else sentence\n",
    "            \n",
    "            # Add remaining chunk\n",
    "            if current_chunk.strip():\n",
    "                chunks.append({\n",
    "                    \"section\": section_title,\n",
    "                    \"content\": current_chunk.strip(),\n",
    "                    \"entities\": entities,  # Include all entities for the last chunk\n",
    "                    \"chunk_id\": len(chunks)\n",
    "                })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def generate_summary_stats(self, sections_dict: Dict) -> pd.DataFrame:\n",
    "        \"\"\"Generate summary statistics for analysis.\"\"\"\n",
    "        stats = []\n",
    "        \n",
    "        for section_title, section_data in sections_dict.items():\n",
    "            preprocessing = section_data[\"preprocessing\"]\n",
    "            \n",
    "            stats.append({\n",
    "                \"Section\": section_title,\n",
    "                \"Word Count\": preprocessing[\"word_count\"],\n",
    "                \"Sentence Count\": preprocessing[\"sentence_count\"],\n",
    "                \"Parties\": len(preprocessing[\"entities\"][\"parties\"]),\n",
    "                \"Dates\": len(preprocessing[\"entities\"][\"dates\"]),\n",
    "                \"Money\": len(preprocessing[\"entities\"][\"money\"]),\n",
    "                \"Locations\": len(preprocessing[\"entities\"][\"locations\"])\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(stats)\n",
    "\n",
    "\n",
    "# ------------------ Example Usage ------------------\n",
    "\n",
    "# Initialize the parser\n",
    "parser = ContractParser()\n",
    "\n",
    "# # Sample contract document\n",
    "# contract_document = \"\"\"\n",
    "# NON-DISCLOSURE AGREEMENT\n",
    "\n",
    "# This Non-Disclosure Agreement (\"Agreement\") is entered into on January 15, 2024, between Acme Corporation, a Delaware corporation (\"Disclosing Party\"), and Beta Technologies Inc., a California corporation (\"Receiving Party\").\n",
    "\n",
    "# 1. DEFINITIONS\n",
    "# This Agreement defines \"Confidential Information\" as any and all non-public, proprietary, or confidential information, including but not limited to technical data, trade secrets, know-how, research, product plans, products, services, customers, customer lists, markets, software, developments, inventions, processes, formulas, technology, designs, drawings, engineering, hardware configuration information, marketing, finances, or other business information disclosed by Acme Corporation.\n",
    "\n",
    "# 2. OBLIGATIONS OF THE RECEIVING PARTY\n",
    "# The Receiving Party hereby agrees to hold and maintain the Confidential Information in strict confidence for a period of five (5) years from the date of disclosure. Beta Technologies Inc. shall not disclose any Confidential Information to third parties without prior written consent.\n",
    "\n",
    "# 3. PAYMENT TERMS\n",
    "# The Receiving Party shall pay a consulting fee of $50,000 within thirty (30) days of delivery of services. Payment shall be made to Acme Corporation's registered office in New York.\n",
    "\n",
    "# ARTICLE IV - TERMINATION\n",
    "# Either party may terminate this Agreement upon thirty (30) days written notice or immediately upon material breach by the other party.\n",
    "\n",
    "# 5. GOVERNING LAW\n",
    "# This Agreement shall be governed by the laws of the State of Delaware.\n",
    "# \"\"\"\n",
    "\n",
    "# Parse the contract\n",
    "print(\"=== PARSING CONTRACT ===\")\n",
    "sections = parser.parse_contract_enhanced(contract_document)\n",
    "\n",
    "# Display parsed sections with preprocessing\n",
    "for title, data in sections.items():\n",
    "    print(f\"\\n--- {title} ---\")\n",
    "    print(f\"Content Length: {len(data['content'])} characters\")\n",
    "    print(f\"Word Count: {data['preprocessing']['word_count']}\")\n",
    "    print(f\"Sentences: {data['preprocessing']['sentence_count']}\")\n",
    "    \n",
    "    print(\"\\nKey Entities:\")\n",
    "    for entity_type, entities in data['preprocessing']['entities'].items():\n",
    "        if entities:\n",
    "            print(f\"  {entity_type.title()}: {[e['text'] for e in entities]}\")\n",
    "    \n",
    "    print(f\"\\nFirst 2 sentences:\")\n",
    "    for i, sentence in enumerate(data['preprocessing']['sentences'][:2]):\n",
    "        print(f\"  {i+1}. {sentence}\")\n",
    "    \n",
    "    print(f\"\\nClean tokens (first 10): {data['preprocessing']['clean_tokens'][:10]}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Generate RAG chunks\n",
    "print(\"\\n=== RAG CHUNKS ===\")\n",
    "chunks = parser.create_rag_chunks(sections, max_chunk_size=300)\n",
    "for i, chunk in enumerate(chunks[:3]):  # Show first 3 chunks\n",
    "    print(f\"\\nChunk {i+1} (Section: {chunk['section']}):\")\n",
    "    print(f\"Content: {chunk['content'][:200]}...\")\n",
    "    # print(f\"Entities: {[e['text'] for e in chunk['entities']['parties']]}\")\n",
    "\n",
    "# Generate summary statistics\n",
    "print(\"\\n=== SUMMARY STATISTICS ===\")\n",
    "stats_df = parser.generate_summary_stats(sections)\n",
    "print(stats_df.to_string(index=False))\n",
    "\n",
    "# Create a DataFrame of all entities for further analysis\n",
    "print(\"\\n=== ALL ENTITIES EXTRACTED ===\")\n",
    "all_entities = []\n",
    "for section_title, section_data in sections.items():\n",
    "    for entity_type, entities in section_data['preprocessing']['entities'].items():\n",
    "        for entity in entities:\n",
    "            all_entities.append({\n",
    "                    'Section': section_title,\n",
    "                'Type': entity_type,\n",
    "                'Text': entity['text'],\n",
    "                'Label': entity['label']\n",
    "            })\n",
    "\n",
    "if all_entities:\n",
    "    entities_df = pd.DataFrame(all_entities)\n",
    "    print(entities_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\n=== PROCESSING COMPLETE ===\")\n",
    "print(f\"Total sections processed: {len(sections)}\")\n",
    "print(f\"Total chunks created: {len(chunks)}\")\n",
    "print(f\"Total entities extracted: {len(all_entities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a0e17a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClauseNode:\n",
    "    def __init__(self, text, section=None, entities=None):\n",
    "        self.text = text\n",
    "        self.section = section\n",
    "        self.entities = entities or []\n",
    "        self.edges = []\n",
    "\n",
    "    def add_edge(self, other_node, relation=\"related\"):\n",
    "        self.edges.append({\"to\": other_node, \"relation\": relation})\n",
    "\n",
    "    def __repr__(self):\n",
    "        # Short preview for printing\n",
    "        return f\"ClauseNode(section='{self.section}', text='{self.text[:50]}...', entities={self.entities})\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6a996e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def parse_sections_with_sentences(text):\n",
    "    section_pattern = re.compile(\n",
    "        r\"(?P<header>(?:^|\\n)(?:\\d+(\\.\\d+)*\\s+.+?|ARTICLE\\s+[IVXLC]+\\s*.*|[A-Z][A-Z\\s\\-]{3,}))\",\n",
    "        re.MULTILINE\n",
    "    )\n",
    "    matches = list(section_pattern.finditer(text))\n",
    "    sections_dict = {}\n",
    "\n",
    "    for i, match in enumerate(matches):\n",
    "        start = match.end()\n",
    "        end = matches[i+1].start() if i+1 < len(matches) else len(text)\n",
    "        header = match.group(\"header\").strip()\n",
    "        content = text[start:end].strip()\n",
    "        doc_nlp = nlp(content)\n",
    "        sentences = [sent.text.strip() for sent in doc_nlp.sents]\n",
    "        entities = [{\"text\": ent.text, \"label\": ent.label_} for ent in doc_nlp.ents]\n",
    "        sections_dict[header] = {\"sentences\": sentences, \"entities\": entities}\n",
    "    return sections_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "917302da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cag(sections_dict):\n",
    "    nodes = []\n",
    "    section_nodes = defaultdict(list)\n",
    "\n",
    "    # Create nodes\n",
    "    for section, data in sections_dict.items():\n",
    "        for sent in data[\"sentences\"]:\n",
    "            entities_in_sent = [e for e in data[\"entities\"] if e[\"text\"] in sent]\n",
    "            node = ClauseNode(text=sent, section=section, entities=entities_in_sent)\n",
    "            nodes.append(node)\n",
    "            section_nodes[section].append(node)\n",
    "\n",
    "    # Connect nodes within the same section\n",
    "    for section, nodes_in_sec in section_nodes.items():\n",
    "        for i in range(len(nodes_in_sec)-1):\n",
    "            nodes_in_sec[i].add_edge(nodes_in_sec[i+1], relation=\"follows\")\n",
    "\n",
    "    # Optional: connect sections sequentially\n",
    "    section_list = list(section_nodes.keys())\n",
    "    for i in range(len(section_list)-1):\n",
    "        last_node_sec = section_nodes[section_list[i]][-1]\n",
    "        first_node_next_sec = section_nodes[section_list[i+1]][0]\n",
    "        last_node_sec.add_edge(first_node_next_sec, relation=\"next_section\")\n",
    "\n",
    "    return nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b2195738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClauseNode(section='ARTICLE IV TERMINATION', text='Either party may terminate this Agreement upon mat...', entities=[{'text': 'Agreement', 'label': 'PRODUCT'}])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sections = parse_sections_with_sentences(contract_document)\n",
    "cag_nodes = build_cag(sections)\n",
    "\n",
    "# Preview nodes and edges\n",
    "for node in cag_nodes:\n",
    "    print(node)\n",
    "    for edge in node.edges:\n",
    "        print(\"   ->\", edge[\"relation\"], \"->\", edge[\"to\"].text[:40])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e5dd07fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section: ARTICLE IV TERMINATION, Saved in: contract_sections/1_ARTICLE IV TERMINATION.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def extract_sections_to_files(doc_text, output_folder=\"sections\"):\n",
    "    \"\"\"\n",
    "    Extract sections from a document and save each section as a separate file.\n",
    "    \"\"\"\n",
    "    # Ensure output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Regex for headings: numbered, ARTICLE, or all caps\n",
    "    section_pattern = re.compile(\n",
    "        r\"(?P<header>(?:^|\\n)(?:\\d+(\\.\\d+)*\\s+.+?|ARTICLE\\s+[IVXLC]+\\s*.*|[A-Z][A-Z\\s\\-]{3,}))\",\n",
    "        re.MULTILINE\n",
    "    )\n",
    "\n",
    "    matches = list(section_pattern.finditer(doc_text))\n",
    "    \n",
    "    sections = []\n",
    "\n",
    "    for i, match in enumerate(matches):\n",
    "        start = match.end()\n",
    "        end = matches[i+1].start() if i+1 < len(matches) else len(doc_text)\n",
    "        header = match.group(\"header\").strip()\n",
    "        content = doc_text[start:end].strip()\n",
    "\n",
    "        # Save to file\n",
    "        safe_header = re.sub(r\"[^\\w\\s]\", \"_\", header)[:50]  # clean file name\n",
    "        file_path = os.path.join(output_folder, f\"{i+1}_{safe_header}.txt\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(content)\n",
    "\n",
    "        sections.append({\"title\": header, \"file\": file_path, \"content\": content})\n",
    "\n",
    "    return sections\n",
    "\n",
    "# ---------------- Example Usage ----------------\n",
    "contract_document = \"\"\"\n",
    "1. DEFINITIONS\n",
    "This Agreement defines \"Confidential Information\" as any business, technical, or financial information disclosed by Acme Ltd.\n",
    "\n",
    "2. OBLIGATIONS OF THE DEVELOPER\n",
    "The Developer shall deliver the software product within 90 days of execution. The Client, Beta Corp, will provide access to all required resources.\n",
    "\n",
    "3. PAYMENT TERMS\n",
    "The Client shall pay $50,000 within 30 days of delivery.\n",
    "\n",
    "ARTICLE IV TERMINATION\n",
    "Either party may terminate this Agreement upon material breach.\n",
    "\"\"\"\n",
    "\n",
    "sections = extract_sections_to_files(contract_document, output_folder=\"contract_sections\")\n",
    "\n",
    "# Print summary\n",
    "for sec in sections:\n",
    "    print(f\"Section: {sec['title']}, Saved in: {sec['file']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e182b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3566/2274962343.py:6: LangChainDeprecationWarning: The class `AzureChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import AzureChatOpenAI``.\n",
      "  llm = AzureChatOpenAI(\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'contract_parser'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 74\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Run the test\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     73\u001b[39m     \u001b[38;5;66;03m# For Jupyter notebook, use:\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m test_contract_analyzer()\n\u001b[32m     76\u001b[39m     \u001b[38;5;66;03m# For regular Python script, use:\u001b[39;00m\n\u001b[32m     77\u001b[39m     result = asyncio.run(test_contract_analyzer())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mtest_contract_analyzer\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      6\u001b[39m llm = AzureChatOpenAI(\n\u001b[32m      7\u001b[39m deployment_name=\u001b[33m\"\u001b[39m\u001b[33mneostats_hackathon_api_v1\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m model=\u001b[33m\"\u001b[39m\u001b[33mgpt-oss-120b\u001b[39m\u001b[33m\"\u001b[39m,                         \n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m azure_endpoint=\u001b[33m\"\u001b[39m\u001b[33mhttps://neoaihackathon.services.ai.azure.com/models/chat/completions?api-version=2024-05-01-preview\u001b[39m\u001b[33m\"\u001b[39m \n\u001b[32m     13\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Initialize the existing contract parser\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcontract_parser\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ContractParser  \u001b[38;5;66;03m# Import your existing parser\u001b[39;00m\n\u001b[32m     17\u001b[39m contract_parser = ContractParser()\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Initialize the multi-agent analyzer\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'contract_parser'"
     ]
    }
   ],
   "source": [
    "# Example usage and testing\n",
    "async def test_contract_analyzer():\n",
    "    \"\"\"Test function for the multi-agent system\"\"\"\n",
    "    \n",
    "    # Initialize your Azure OpenAI LLM\n",
    "    llm = AzureChatOpenAI(\n",
    "    deployment_name=\"neostats_hackathon_api_v1\",\n",
    "    model=\"gpt-oss-120b\",                         \n",
    "    temperature=0,\n",
    "    api_version=\"2024-05-01-preview\",             \n",
    "    api_key=\"Azure_Open_API\",                      # Replace with your actual key\n",
    "    azure_endpoint=\"https://neoaihackathon.services.ai.azure.com/models/chat/completions?api-version=2024-05-01-preview\" \n",
    "    )\n",
    "    \n",
    "    # Initialize the existing contract parser\n",
    "    from contract_parser import ContractParser  # Import your existing parser\n",
    "    contract_parser = ContractParser()\n",
    "    \n",
    "    # Initialize the multi-agent analyzer\n",
    "    analyzer = MultiAgentContractAnalyzer(llm, contract_parser)\n",
    "    \n",
    "    # Sample contract for testing\n",
    "    contract = contract_document.copy()  # \n",
    "\n",
    "    \n",
    "    # Run the analysis\n",
    "    try:\n",
    "        result = await analyzer.analyze_contract(contract)\n",
    "        \n",
    "        # Display results in a dashboard-friendly format\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"CONTRACT ANALYSIS DASHBOARD DATA\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        print(f\"\\nüìä EXECUTIVE SUMMARY:\")\n",
    "        print(f\"Document ID: {result['document_id']}\")\n",
    "        print(f\"Analysis Time: {result['timestamp']}\")\n",
    "        print(f\"Overall Risk Score: {result['risk_assessment']['overall_risk_score']:.2f}\")\n",
    "        print(f\"Contract Type: {result['insights']['contract_type']}\")\n",
    "        \n",
    "        print(f\"\\nüéØ KEY METRICS:\")\n",
    "        risk_dist = result['risk_assessment']['risk_distribution']\n",
    "        print(f\"High Risk Clauses: {risk_dist['HIGH']}\")\n",
    "        print(f\"Medium Risk Clauses: {risk_dist['MEDIUM']}\")\n",
    "        print(f\"Low Risk Clauses: {risk_dist['LOW']}\")\n",
    "        \n",
    "        print(f\"\\nüë• PARTIES INVOLVED:\")\n",
    "        for i, party in enumerate(result['parties'], 1):\n",
    "            print(f\"{i}. {party['name']} ({party['role']})\")\n",
    "        \n",
    "        print(f\"\\n‚ö†Ô∏è  HIGH RISK AREAS:\")\n",
    "        for area in result['risk_assessment']['high_risk_areas']:\n",
    "            print(f\"‚Ä¢ {area}\")\n",
    "        \n",
    "        print(f\"\\nüí° KEY RECOMMENDATIONS:\")\n",
    "        for rec in result['recommendations']:\n",
    "            print(f\"‚Ä¢ {rec}\")\n",
    "        \n",
    "        # Export as JSON for dashboard integration\n",
    "        with open(f\"contract_analysis_{result['document_id']}.json\", \"w\") as f:\n",
    "            json.dump(result, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"\\nüíæ Full analysis exported to: contract_analysis_{result['document_id']}.json\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during analysis: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run the test\n",
    "if __name__ == \"__main__\":\n",
    "    # For Jupyter notebook, use:\n",
    "    result = await test_contract_analyzer()\n",
    "    \n",
    "    # For regular Python script, use:\n",
    "    result = asyncio.run(test_contract_analyzer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ad9c853d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "from enum import Enum\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import concurrent.futures\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "import re\n",
    "import numpy as np\n",
    "import spacy\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import unicodedata\n",
    "\n",
    "# Load spaCy model with custom configuration\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"‚ö†Ô∏è  spaCy model 'en_core_web_sm' not found. Installing...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class ContractParser:\n",
    "    \"\"\"\n",
    "    Enhanced contract parser with comprehensive preprocessing for RAG systems.\n",
    "    Handles section detection, text cleaning, and NLP preprocessing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Enhanced section patterns for various legal document formats\n",
    "        self.section_patterns = [\n",
    "            # Numbered sections: \"1.\", \"1.1\", \"1.1.1\", etc.\n",
    "            r\"(?P<header>^(?:\\d+(?:\\.\\d+)*\\.?\\s+[A-Z][^\\n]*))(?=\\n)\",\n",
    "            \n",
    "            # Roman numerals: \"I.\", \"II.\", \"III.\", etc.\n",
    "            r\"(?P<header>^(?:[IVXLCDM]+\\.?\\s+[A-Z][^\\n]*))(?=\\n)\",\n",
    "            \n",
    "            # Articles: \"ARTICLE I\", \"ARTICLE 1\", etc.\n",
    "            r\"(?P<header>^(?:ARTICLE\\s+(?:[IVXLCDM]+|\\d+)[\\s\\-]*[^\\n]*))(?=\\n)\",\n",
    "            \n",
    "            # All caps headings (minimum 3 words)\n",
    "            r\"(?P<header>^(?:[A-Z][A-Z\\s\\-\\,\\&]{8,}))(?=\\n)\",\n",
    "            \n",
    "            # Section with keywords\n",
    "            r\"(?P<header>^(?:SECTION|CLAUSE|PART|CHAPTER)\\s+\\d+[\\s\\-]*[^\\n]*)(?=\\n)\",\n",
    "            \n",
    "            # Lettered sections: \"(a)\", \"(A)\", \"a)\", \"A)\", etc.\n",
    "            r\"(?P<header>^(?:\\([a-zA-Z]\\)|\\b[a-zA-Z]\\)\\s+)[A-Z][^\\n]*)(?=\\n)\",\n",
    "        ]\n",
    "        \n",
    "        # Compile all patterns\n",
    "        self.compiled_patterns = [re.compile(pattern, re.MULTILINE | re.IGNORECASE) \n",
    "                                for pattern in self.section_patterns]\n",
    "        \n",
    "        # Legal stopwords to remove during preprocessing\n",
    "        self.legal_stopwords = {\n",
    "            'shall', 'whereas', 'hereby', 'herein', 'hereof', 'hereto', 'herewith',\n",
    "            'hereunder', 'therefor', 'thereof', 'thereto', 'therein', 'whereby',\n",
    "            'aforesaid', 'aforementioned', 'said', 'such', 'same'\n",
    "        }\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text content.\"\"\"\n",
    "        # Normalize unicode characters\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        \n",
    "        # Remove excessive whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Remove page breaks and form feeds\n",
    "        text = re.sub(r'[\\f\\r\\v]', '\\n', text)\n",
    "        \n",
    "        # Standardize quotes\n",
    "        text = re.sub(r'[\"\"\"]', '\"', text)\n",
    "        text = re.sub(r\"[''']\", \"'\", text)\n",
    "        \n",
    "        # Remove excessive line breaks\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def detect_sections(self, text: str) -> List[Tuple[str, int, int]]:\n",
    "        \"\"\"\n",
    "        Detect sections in the document.\n",
    "        Returns list of tuples: (header, start_pos, end_pos)\n",
    "        \"\"\"\n",
    "        all_matches = []\n",
    "        \n",
    "        for pattern in self.compiled_patterns:\n",
    "            matches = list(pattern.finditer(text))\n",
    "            for match in matches:\n",
    "                header = match.group(\"header\").strip()\n",
    "                # Clean header\n",
    "                header = re.sub(r'\\s+', ' ', header)\n",
    "                all_matches.append((header, match.start(), match.end()))\n",
    "        \n",
    "        # Sort by position and remove duplicates\n",
    "        all_matches.sort(key=lambda x: x[1])\n",
    "        \n",
    "        # Remove overlapping matches (keep the first one)\n",
    "        filtered_matches = []\n",
    "        last_end = -1\n",
    "        for match in all_matches:\n",
    "            if match[1] >= last_end:\n",
    "                filtered_matches.append(match)\n",
    "                last_end = match[2]\n",
    "        \n",
    "        return filtered_matches\n",
    "    \n",
    "    def extract_entities_enhanced(self, doc) -> Dict[str, List]:\n",
    "        \"\"\"Extract and categorize named entities with legal context.\"\"\"\n",
    "        entities = {\n",
    "            'parties': [],      # PERSON, ORG\n",
    "            'locations': [],    # GPE, LOC\n",
    "            'dates': [],        # DATE\n",
    "            'money': [],        # MONEY\n",
    "            'laws': [],         # LAW\n",
    "            'other': []         # Miscellaneous\n",
    "        }\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            entity_info = {\n",
    "                \"text\": ent.text.strip(),\n",
    "                \"label\": ent.label_,\n",
    "                \"start\": ent.start_char,\n",
    "                \"end\": ent.end_char\n",
    "            }\n",
    "            \n",
    "            if ent.label_ in ['PERSON', 'ORG']:\n",
    "                entities['parties'].append(entity_info)\n",
    "            elif ent.label_ in ['GPE', 'LOC']:\n",
    "                entities['locations'].append(entity_info)\n",
    "            elif ent.label_ == 'DATE':\n",
    "                entities['dates'].append(entity_info)\n",
    "            elif ent.label_ == 'MONEY':\n",
    "                entities['money'].append(entity_info)\n",
    "            elif ent.label_ == 'LAW':\n",
    "                entities['laws'].append(entity_info)\n",
    "            else:\n",
    "                entities['other'].append(entity_info)\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def preprocess_for_rag(self, text: str) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        Comprehensive preprocessing for RAG system.\n",
    "        Returns tokens, lemmas, entities, and other features.\n",
    "        \"\"\"\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Extract tokens with detailed information\n",
    "        tokens = []\n",
    "        lemmas = []\n",
    "        clean_tokens = []\n",
    "        \n",
    "        for token in doc:\n",
    "            # Skip punctuation, whitespace, and stop words for clean tokens\n",
    "            if not token.is_punct and not token.is_space and not token.is_stop:\n",
    "                # Additional filter for legal documents\n",
    "                if token.lemma_.lower() not in self.legal_stopwords:\n",
    "                    clean_tokens.append(token.lemma_.lower())\n",
    "                    lemmas.append(token.lemma_)\n",
    "            \n",
    "            # Keep all tokens for reference\n",
    "            tokens.append({\n",
    "                'text': token.text,\n",
    "                'lemma': token.lemma_,\n",
    "                'pos': token.pos_,\n",
    "                'tag': token.tag_,\n",
    "                'is_alpha': token.is_alpha,\n",
    "                'is_stop': token.is_stop\n",
    "            })\n",
    "        \n",
    "        # Extract sentences\n",
    "        sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "        \n",
    "        # Extract entities\n",
    "        entities = self.extract_entities_enhanced(doc)\n",
    "        \n",
    "        # Extract noun phrases\n",
    "        noun_phrases = [chunk.text for chunk in doc.noun_chunks]\n",
    "        \n",
    "        return {\n",
    "            'tokens': tokens,\n",
    "            'lemmas': lemmas,\n",
    "            'clean_tokens': clean_tokens,\n",
    "            'sentences': sentences,\n",
    "            'entities': entities,\n",
    "            'noun_phrases': noun_phrases,\n",
    "            'word_count': len([t for t in doc if not t.is_punct and not t.is_space]),\n",
    "            'sentence_count': len(sentences)\n",
    "        }\n",
    "    \n",
    "    def parse_contract_enhanced(self, text: str) -> Dict[str, Dict]:\n",
    "        \"\"\"\n",
    "        Main parsing function with comprehensive preprocessing.\n",
    "        \"\"\"\n",
    "        # Clean the input text\n",
    "        text = self.clean_text(text)\n",
    "        \n",
    "        # Detect sections\n",
    "        section_matches = self.detect_sections(text)\n",
    "        \n",
    "        if not section_matches:\n",
    "            # If no sections found, treat entire text as one section\n",
    "            return {\n",
    "                \"FULL_DOCUMENT\": {\n",
    "                    \"content\": text,\n",
    "                    \"preprocessing\": self.preprocess_for_rag(text)\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        sections_dict = {}\n",
    "        \n",
    "        for i, (header, start, end) in enumerate(section_matches):\n",
    "            # Determine content boundaries\n",
    "            content_start = end\n",
    "            content_end = section_matches[i+1][1] if i+1 < len(section_matches) else len(text)\n",
    "            \n",
    "            # Extract content\n",
    "            content = text[content_start:content_end].strip()\n",
    "            \n",
    "            if content:  # Only process non-empty sections\n",
    "                sections_dict[header] = {\n",
    "                    \"content\": content,\n",
    "                    \"preprocessing\": self.preprocess_for_rag(content)\n",
    "                }\n",
    "        \n",
    "        return sections_dict\n",
    "    \n",
    "    def create_rag_chunks(self, sections_dict: Dict, max_chunk_size: int = 500) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Create optimized chunks for RAG system based on sentences.\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        for section_title, section_data in sections_dict.items():\n",
    "            sentences = section_data[\"preprocessing\"][\"sentences\"]\n",
    "            entities = section_data[\"preprocessing\"][\"entities\"]\n",
    "            \n",
    "            current_chunk = \"\"\n",
    "            current_entities = []\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                # Check if adding this sentence exceeds chunk size\n",
    "                if len(current_chunk) + len(sentence) > max_chunk_size and current_chunk:\n",
    "                    # Save current chunk\n",
    "                    chunks.append({\n",
    "                        \"section\": section_title,\n",
    "                        \"content\": current_chunk.strip(),\n",
    "                        \"entities\": current_entities,\n",
    "                        \"chunk_id\": len(chunks)\n",
    "                    })\n",
    "                    current_chunk = sentence\n",
    "                    current_entities = []\n",
    "                else:\n",
    "                    current_chunk += \" \" + sentence if current_chunk else sentence\n",
    "            \n",
    "            # Add remaining chunk\n",
    "            if current_chunk.strip():\n",
    "                chunks.append({\n",
    "                    \"section\": section_title,\n",
    "                    \"content\": current_chunk.strip(),\n",
    "                    \"entities\": entities,  # Include all entities for the last chunk\n",
    "                    \"chunk_id\": len(chunks)\n",
    "                })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def generate_summary_stats(self, sections_dict: Dict) -> pd.DataFrame:\n",
    "        \"\"\"Generate summary statistics for analysis.\"\"\"\n",
    "        stats = []\n",
    "        \n",
    "        for section_title, section_data in sections_dict.items():\n",
    "            preprocessing = section_data[\"preprocessing\"]\n",
    "            \n",
    "            stats.append({\n",
    "                \"Section\": section_title,\n",
    "                \"Word Count\": preprocessing[\"word_count\"],\n",
    "                \"Sentence Count\": preprocessing[\"sentence_count\"],\n",
    "                \"Parties\": len(preprocessing[\"entities\"][\"parties\"]),\n",
    "                \"Dates\": len(preprocessing[\"entities\"][\"dates\"]),\n",
    "                \"Money\": len(preprocessing[\"entities\"][\"money\"]),\n",
    "                \"Locations\": len(preprocessing[\"entities\"][\"locations\"])\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(stats)\n",
    "\n",
    "def dynamic_chunker(text: str, C_min=400, C_max=2000, alpha=30, beta=0.20):\n",
    "    \"\"\"Dynamic chunking function\"\"\"\n",
    "    N = len(text)\n",
    "    chunk_size = min(C_max, max(C_min, int(alpha * (N ** 0.6))))\n",
    "    overlap = int(beta * chunk_size)\n",
    "    \n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    chunks = splitter.split_text(text)\n",
    "    return chunks, chunk_size, overlap\n",
    "\n",
    "# Risk levels enumeration\n",
    "class RiskLevel(Enum):\n",
    "    HIGH = \"HIGH\"\n",
    "    MEDIUM = \"MEDIUM\"\n",
    "    LOW = \"LOW\"\n",
    "\n",
    "@dataclass\n",
    "class ClauseAnalysis:\n",
    "    clause_id: str\n",
    "    section: str\n",
    "    content: str\n",
    "    risk_level: RiskLevel\n",
    "    risk_score: float\n",
    "    ambiguity_flags: List[str]\n",
    "    compliance_issues: List[str]\n",
    "    recommendations: List[str]\n",
    "\n",
    "@dataclass\n",
    "class PartyInfo:\n",
    "    name: str\n",
    "    role: str\n",
    "    stakes: List[str]\n",
    "    obligations: List[str]\n",
    "    rights: List[str]\n",
    "\n",
    "@dataclass\n",
    "class ContractInsights:\n",
    "    contract_type: str\n",
    "    key_themes: List[str]\n",
    "    critical_dates: List[str]\n",
    "    financial_terms: List[str]\n",
    "    jurisdiction: str\n",
    "    termination_clauses: List[str]\n",
    "\n",
    "@dataclass\n",
    "class RiskAssessment:\n",
    "    overall_risk_score: float\n",
    "    risk_distribution: Dict[str, int]\n",
    "    high_risk_areas: List[str]\n",
    "    compliance_gaps: List[str]\n",
    "    mitigation_strategies: List[str]\n",
    "\n",
    "@dataclass\n",
    "class FinalAnalysis:\n",
    "    document_id: str\n",
    "    timestamp: str\n",
    "    is_contract: bool\n",
    "    confidence_score: float\n",
    "    risk_assessment: RiskAssessment\n",
    "    insights: ContractInsights\n",
    "    parties: List[PartyInfo]\n",
    "    clause_analyses: List[ClauseAnalysis]\n",
    "    executive_summary: str\n",
    "    recommendations: List[str]\n",
    "\n",
    "class ContractAnalysisAgent:\n",
    "    \"\"\"Base class for all analysis agents\"\"\"\n",
    "    \n",
    "    def __init__(self, llm: AzureChatOpenAI, name: str):\n",
    "        self.llm = llm\n",
    "        self.name = name\n",
    "        \n",
    "    async def analyze(self, data: Any) -> Any:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class DocumentClassificationAgent(ContractAnalysisAgent):\n",
    "    \"\"\"Agent 1: Determines if document is contract-related\"\"\"\n",
    "    \n",
    "    def __init__(self, llm: AzureChatOpenAI):\n",
    "        super().__init__(llm, \"DocumentClassifier\")\n",
    "        \n",
    "    async def analyze(self, text: str) -> Tuple[bool, float, str]:\n",
    "        prompt = f\"\"\"\n",
    "        Analyze the following document and determine if it's a contract or legal agreement.\n",
    "        \n",
    "        Document text: {text[:2000]}...\n",
    "        \n",
    "        Consider these factors:\n",
    "        1. Presence of parties (entities entering agreement)\n",
    "        2. Terms and conditions\n",
    "        3. Obligations and rights\n",
    "        4. Legal language patterns\n",
    "        5. Structure typical of contracts\n",
    "        \n",
    "        Respond in JSON format:\n",
    "        {{\n",
    "            \"is_contract\": true/false,\n",
    "            \"confidence\": 0.0-1.0,\n",
    "            \"reasoning\": \"explanation for classification\",\n",
    "            \"document_type\": \"estimated document type\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \n",
    "        messages = [\n",
    "            SystemMessage(content=\"You are an expert legal document classifier.\"),\n",
    "            HumanMessage(content=prompt)\n",
    "        ]\n",
    "        \n",
    "        response = await self.llm.ainvoke(messages)\n",
    "        result = json.loads(response.content)\n",
    "        \n",
    "        return result[\"is_contract\"], result[\"confidence\"], result[\"reasoning\"]\n",
    "\n",
    "class RiskAssessmentAgent(ContractAnalysisAgent):\n",
    "    \"\"\"Agent 2.1: Assesses risks in contract clauses\"\"\"\n",
    "    \n",
    "    def __init__(self, llm: AzureChatOpenAI):\n",
    "        super().__init__(llm, \"RiskAssessor\")\n",
    "        \n",
    "    async def analyze(self, sections: Dict[str, Dict]) -> List[ClauseAnalysis]:\n",
    "        clause_analyses = []\n",
    "        \n",
    "        for section_title, section_data in sections.items():\n",
    "            content = section_data[\"content\"]\n",
    "            \n",
    "            prompt = f\"\"\"\n",
    "            Analyze the following contract section for legal and business risks:\n",
    "            \n",
    "            Section: {section_title}\n",
    "            Content: {content}\n",
    "            \n",
    "            Identify:\n",
    "            1. Risk level (HIGH/MEDIUM/LOW)\n",
    "            2. Risk score (0.0-1.0)\n",
    "            3. Ambiguous or one-sided clauses\n",
    "            4. Potential compliance issues\n",
    "            5. Recommendations for risk mitigation\n",
    "            \n",
    "            Respond in JSON format:\n",
    "            {{\n",
    "                \"risk_level\": \"HIGH/MEDIUM/LOW\",\n",
    "                \"risk_score\": 0.0-1.0,\n",
    "                \"ambiguity_flags\": [\"flag1\", \"flag2\"],\n",
    "                \"compliance_issues\": [\"issue1\", \"issue2\"],\n",
    "                \"recommendations\": [\"rec1\", \"rec2\"]\n",
    "            }}\n",
    "            \"\"\"\n",
    "            \n",
    "            messages = [\n",
    "                SystemMessage(content=\"You are an expert contract risk analyst.\"),\n",
    "                HumanMessage(content=prompt)\n",
    "            ]\n",
    "            \n",
    "            response = await self.llm.ainvoke(messages)\n",
    "            result = json.loads(response.content)\n",
    "            \n",
    "            clause_analysis = ClauseAnalysis(\n",
    "                clause_id=f\"clause_{len(clause_analyses)}\",\n",
    "                section=section_title,\n",
    "                content=content[:500] + \"...\" if len(content) > 500 else content,\n",
    "                risk_level=RiskLevel(result[\"risk_level\"]),\n",
    "                risk_score=result[\"risk_score\"],\n",
    "                ambiguity_flags=result[\"ambiguity_flags\"],\n",
    "                compliance_issues=result[\"compliance_issues\"],\n",
    "                recommendations=result[\"recommendations\"]\n",
    "            )\n",
    "            \n",
    "            clause_analyses.append(clause_analysis)\n",
    "            \n",
    "        return clause_analyses\n",
    "\n",
    "class InsightsAgent(ContractAnalysisAgent):\n",
    "    \"\"\"Agent 2.2: Extracts key insights and themes\"\"\"\n",
    "    \n",
    "    def __init__(self, llm: AzureChatOpenAI):\n",
    "        super().__init__(llm, \"InsightsExtractor\")\n",
    "        \n",
    "    async def analyze(self, sections: Dict[str, Dict], entities: List[Dict]) -> ContractInsights:\n",
    "        # Combine all content for analysis\n",
    "        full_text = \" \".join([section[\"content\"] for section in sections.values()])\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Analyze this contract for key insights and themes:\n",
    "        \n",
    "        Contract text: {full_text[:3000]}...\n",
    "        \n",
    "        Entities found: {entities[:20]}\n",
    "        \n",
    "        Extract:\n",
    "        1. Contract type (NDA, Employment, Service Agreement, etc.)\n",
    "        2. Key themes and focus areas\n",
    "        3. Critical dates and deadlines\n",
    "        4. Financial terms and amounts\n",
    "        5. Governing jurisdiction\n",
    "        6. Termination and renewal clauses\n",
    "        \n",
    "        Respond in JSON format:\n",
    "        {{\n",
    "            \"contract_type\": \"type\",\n",
    "            \"key_themes\": [\"theme1\", \"theme2\"],\n",
    "            \"critical_dates\": [\"date1\", \"date2\"],\n",
    "            \"financial_terms\": [\"term1\", \"term2\"],\n",
    "            \"jurisdiction\": \"jurisdiction\",\n",
    "            \"termination_clauses\": [\"clause1\", \"clause2\"]\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \n",
    "        messages = [\n",
    "            SystemMessage(content=\"You are an expert contract analyst specializing in extracting key insights.\"),\n",
    "            HumanMessage(content=prompt)\n",
    "        ]\n",
    "        \n",
    "        response = await self.llm.ainvoke(messages)\n",
    "        result = json.loads(response.content)\n",
    "        \n",
    "        return ContractInsights(\n",
    "            contract_type=result[\"contract_type\"],\n",
    "            key_themes=result[\"key_themes\"],\n",
    "            critical_dates=result[\"critical_dates\"],\n",
    "            financial_terms=result[\"financial_terms\"],\n",
    "            jurisdiction=result[\"jurisdiction\"],\n",
    "            termination_clauses=result[\"termination_clauses\"]\n",
    "        )\n",
    "\n",
    "class PartiesAgent(ContractAnalysisAgent):\n",
    "    \"\"\"Agent 2.3: Identifies parties and their stakes\"\"\"\n",
    "    \n",
    "    def __init__(self, llm: AzureChatOpenAI):\n",
    "        super().__init__(llm, \"PartiesAnalyzer\")\n",
    "        \n",
    "    async def analyze(self, sections: Dict[str, Dict], entities: List[Dict]) -> List[PartyInfo]:\n",
    "        # Extract party-related entities\n",
    "        parties_entities = [e for e in entities if e.get('Type') in ['parties']]\n",
    "        \n",
    "        full_text = \" \".join([section[\"content\"] for section in sections.values()])\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Identify all parties involved in this contract and analyze their stakes:\n",
    "        \n",
    "        Contract text: {full_text[:3000]}...\n",
    "        \n",
    "        Identified party entities: {parties_entities}\n",
    "        \n",
    "        For each party, determine:\n",
    "        1. Name and role in the contract\n",
    "        2. Key stakes and interests\n",
    "        3. Main obligations\n",
    "        4. Rights and benefits\n",
    "        \n",
    "        Respond in JSON format:\n",
    "        {{\n",
    "            \"parties\": [\n",
    "                {{\n",
    "                    \"name\": \"Party Name\",\n",
    "                    \"role\": \"Role (e.g., Service Provider, Client)\",\n",
    "                    \"stakes\": [\"stake1\", \"stake2\"],\n",
    "                    \"obligations\": [\"obligation1\", \"obligation2\"],\n",
    "                    \"rights\": [\"right1\", \"right2\"]\n",
    "                }}\n",
    "            ]\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \n",
    "        messages = [\n",
    "            SystemMessage(content=\"You are an expert in contract party analysis.\"),\n",
    "            HumanMessage(content=prompt)\n",
    "        ]\n",
    "        \n",
    "        response = await self.llm.ainvoke(messages)\n",
    "        result = json.loads(response.content)\n",
    "        \n",
    "        parties = []\n",
    "        for party_data in result[\"parties\"]:\n",
    "            party = PartyInfo(\n",
    "                name=party_data[\"name\"],\n",
    "                role=party_data[\"role\"],\n",
    "                stakes=party_data[\"stakes\"],\n",
    "                obligations=party_data[\"obligations\"],\n",
    "                rights=party_data[\"rights\"]\n",
    "            )\n",
    "            parties.append(party)\n",
    "            \n",
    "        return parties\n",
    "\n",
    "class MetricsAgent(ContractAnalysisAgent):\n",
    "    \"\"\"Agent 3.1: Calculates risk scores and metrics\"\"\"\n",
    "    \n",
    "    def __init__(self, llm: AzureChatOpenAI):\n",
    "        super().__init__(llm, \"MetricsCalculator\")\n",
    "        \n",
    "    async def analyze(self, clause_analyses: List[ClauseAnalysis]) -> RiskAssessment:\n",
    "        # Calculate overall risk score\n",
    "        risk_scores = [clause.risk_score for clause in clause_analyses]\n",
    "        overall_risk_score = np.mean(risk_scores) if risk_scores else 0.0\n",
    "        \n",
    "        # Risk distribution\n",
    "        risk_distribution = {\n",
    "            \"HIGH\": len([c for c in clause_analyses if c.risk_level == RiskLevel.HIGH]),\n",
    "            \"MEDIUM\": len([c for c in clause_analyses if c.risk_level == RiskLevel.MEDIUM]),\n",
    "            \"LOW\": len([c for c in clause_analyses if c.risk_level == RiskLevel.LOW])\n",
    "        }\n",
    "        \n",
    "        # High risk areas\n",
    "        high_risk_areas = [\n",
    "            clause.section for clause in clause_analyses \n",
    "            if clause.risk_level == RiskLevel.HIGH\n",
    "        ]\n",
    "        \n",
    "        # Compile compliance gaps\n",
    "        compliance_gaps = []\n",
    "        for clause in clause_analyses:\n",
    "            compliance_gaps.extend(clause.compliance_issues)\n",
    "        \n",
    "        # Generate mitigation strategies\n",
    "        mitigation_prompt = f\"\"\"\n",
    "        Based on the following risk assessment data, provide strategic recommendations:\n",
    "        \n",
    "        Overall Risk Score: {overall_risk_score}\n",
    "        High Risk Areas: {high_risk_areas}\n",
    "        Compliance Gaps: {compliance_gaps[:10]}\n",
    "        \n",
    "        Provide 5 key mitigation strategies in JSON format:\n",
    "        {{\n",
    "            \"strategies\": [\"strategy1\", \"strategy2\", \"strategy3\", \"strategy4\", \"strategy5\"]\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \n",
    "        messages = [\n",
    "            SystemMessage(content=\"You are a legal risk mitigation expert.\"),\n",
    "            HumanMessage(content=mitigation_prompt)\n",
    "        ]\n",
    "        \n",
    "        response = await self.llm.ainvoke(messages)\n",
    "        result = json.loads(response.content)\n",
    "        \n",
    "        return RiskAssessment(\n",
    "            overall_risk_score=overall_risk_score,\n",
    "            risk_distribution=risk_distribution,\n",
    "            high_risk_areas=high_risk_areas,\n",
    "            compliance_gaps=list(set(compliance_gaps))[:10],  # Unique, limited list\n",
    "            mitigation_strategies=result[\"strategies\"]\n",
    "        )\n",
    "\n",
    "class EvaluationAgent(ContractAnalysisAgent):\n",
    "    \"\"\"Agent 3.2: Evaluates and validates results\"\"\"\n",
    "    \n",
    "    def __init__(self, llm: AzureChatOpenAI):\n",
    "        super().__init__(llm, \"ResultEvaluator\")\n",
    "        \n",
    "    async def analyze(self, preliminary_analysis: Dict) -> Dict[str, Any]:\n",
    "        prompt = f\"\"\"\n",
    "        Evaluate the quality and completeness of this contract analysis:\n",
    "        \n",
    "        Analysis summary:\n",
    "        - Risk Score: {preliminary_analysis.get('overall_risk_score', 0)}\n",
    "        - Number of clauses analyzed: {len(preliminary_analysis.get('clause_analyses', []))}\n",
    "        - Parties identified: {len(preliminary_analysis.get('parties', []))}\n",
    "        - High risk areas: {preliminary_analysis.get('high_risk_areas', [])}\n",
    "        \n",
    "        Provide:\n",
    "        1. Quality score (0.0-1.0)\n",
    "        2. Completeness assessment\n",
    "        3. Confidence level in analysis\n",
    "        4. Areas for improvement\n",
    "        \n",
    "        Respond in JSON format:\n",
    "        {{\n",
    "            \"quality_score\": 0.0-1.0,\n",
    "            \"completeness\": \"HIGH/MEDIUM/LOW\",\n",
    "            \"confidence\": 0.0-1.0,\n",
    "            \"improvements\": [\"improvement1\", \"improvement2\"]\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \n",
    "        messages = [\n",
    "            SystemMessage(content=\"You are a quality assurance expert for legal document analysis.\"),\n",
    "            HumanMessage(content=prompt)\n",
    "        ]\n",
    "        \n",
    "        response = await self.llm.ainvoke(messages)\n",
    "        return json.loads(response.content)\n",
    "\n",
    "class FinalReportAgent(ContractAnalysisAgent):\n",
    "    \"\"\"Agent 4: Generates final JSON report\"\"\"\n",
    "    \n",
    "    def __init__(self, llm: AzureChatOpenAI):\n",
    "        super().__init__(llm, \"FinalReporter\")\n",
    "        \n",
    "    async def analyze(self, analysis_components: Dict) -> FinalAnalysis:\n",
    "        # Generate executive summary\n",
    "        summary_prompt = f\"\"\"\n",
    "        Create an executive summary for this contract analysis:\n",
    "        \n",
    "        Key findings:\n",
    "        - Contract type: {analysis_components['insights'].contract_type}\n",
    "        - Overall risk score: {analysis_components['risk_assessment'].overall_risk_score}\n",
    "        - High risk areas: {analysis_components['risk_assessment'].high_risk_areas}\n",
    "        - Number of parties: {len(analysis_components['parties'])}\n",
    "        \n",
    "        Provide a concise 2-3 sentence executive summary focusing on the most critical findings.\n",
    "        \n",
    "        Respond in JSON format:\n",
    "        {{\n",
    "            \"executive_summary\": \"summary text\",\n",
    "            \"key_recommendations\": [\"rec1\", \"rec2\", \"rec3\"]\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \n",
    "        messages = [\n",
    "            SystemMessage(content=\"You are an executive report writer specializing in contract analysis summaries.\"),\n",
    "            HumanMessage(content=summary_prompt)\n",
    "        ]\n",
    "        \n",
    "        response = await self.llm.ainvoke(messages)\n",
    "        result = json.loads(response.content)\n",
    "        \n",
    "        return FinalAnalysis(\n",
    "            document_id=f\"contract_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            is_contract=analysis_components['is_contract'],\n",
    "            confidence_score=analysis_components['confidence'],\n",
    "            risk_assessment=analysis_components['risk_assessment'],\n",
    "            insights=analysis_components['insights'],\n",
    "            parties=analysis_components['parties'],\n",
    "            clause_analyses=analysis_components['clause_analyses'],\n",
    "            executive_summary=result[\"executive_summary\"],\n",
    "            recommendations=result[\"key_recommendations\"]\n",
    "        )\n",
    "\n",
    "class MultiAgentContractAnalyzer:\n",
    "    \"\"\"Main orchestrator for the multi-agent system\"\"\"\n",
    "    \n",
    "    def __init__(self, llm: AzureChatOpenAI, contract_parser):\n",
    "        self.llm = llm\n",
    "        self.parser = contract_parser\n",
    "        \n",
    "        # Initialize agents\n",
    "        self.classifier = DocumentClassificationAgent(llm)\n",
    "        self.risk_assessor = RiskAssessmentAgent(llm)\n",
    "        self.insights_agent = InsightsAgent(llm)\n",
    "        self.parties_agent = PartiesAgent(llm)\n",
    "        self.metrics_agent = MetricsAgent(llm)\n",
    "        self.evaluator = EvaluationAgent(llm)\n",
    "        self.reporter = FinalReportAgent(llm)\n",
    "        \n",
    "        # Setup RAG components\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        )\n",
    "        \n",
    "    def dynamic_chunker(self, text: str, C_min=400, C_max=2000, alpha=30, beta=0.20):\n",
    "        \"\"\"Dynamic chunking function from your existing code\"\"\"\n",
    "        N = len(text)\n",
    "        chunk_size = min(C_max, max(C_min, int(alpha * (N ** 0.6))))\n",
    "        overlap = int(beta * chunk_size)\n",
    "        \n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "        )\n",
    "        \n",
    "        chunks = splitter.split_text(text)\n",
    "        return chunks, chunk_size, overlap\n",
    "    \n",
    "    async def analyze_contract(self, contract_text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Main analysis pipeline with parallel processing\"\"\"\n",
    "        \n",
    "        print(\"üîç Starting contract analysis...\")\n",
    "        \n",
    "        # Step 1: Document Classification\n",
    "        print(\"üìã Classifying document...\")\n",
    "        is_contract, confidence, reasoning = await self.classifier.analyze(contract_text)\n",
    "        \n",
    "        if not is_contract or confidence < 0.7:\n",
    "            return {\n",
    "                \"error\": \"Document does not appear to be a contract\",\n",
    "                \"confidence\": confidence,\n",
    "                \"reasoning\": reasoning\n",
    "            }\n",
    "        \n",
    "        # Step 2: Parse contract using existing parser\n",
    "        print(\"üìÑ Parsing contract sections...\")\n",
    "        sections = self.parser.parse_contract_enhanced(contract_text)\n",
    "        \n",
    "        # Extract all entities for analysis\n",
    "        all_entities = []\n",
    "        for section_title, section_data in sections.items():\n",
    "            for entity_type, entities in section_data['preprocessing']['entities'].items():\n",
    "                for entity in entities:\n",
    "                    all_entities.append({\n",
    "                        'Section': section_title,\n",
    "                        'Type': entity_type,\n",
    "                        'Text': entity['text'],\n",
    "                        'Label': entity['label']\n",
    "                    })\n",
    "        \n",
    "        # Step 3: Parallel analysis by specialized agents\n",
    "        print(\"üîÑ Running parallel analysis...\")\n",
    "        \n",
    "        async def run_parallel_analysis():\n",
    "            tasks = [\n",
    "                self.risk_assessor.analyze(sections),\n",
    "                self.insights_agent.analyze(sections, all_entities),\n",
    "                self.parties_agent.analyze(sections, all_entities)\n",
    "            ]\n",
    "            \n",
    "            return await asyncio.gather(*tasks)\n",
    "        \n",
    "        clause_analyses, insights, parties = await run_parallel_analysis()\n",
    "        \n",
    "        # Step 4: Calculate metrics and evaluate\n",
    "        print(\"üìä Calculating metrics...\")\n",
    "        risk_assessment = await self.metrics_agent.analyze(clause_analyses)\n",
    "        \n",
    "        # Prepare preliminary analysis for evaluation\n",
    "        preliminary_analysis = {\n",
    "            'overall_risk_score': risk_assessment.overall_risk_score,\n",
    "            'clause_analyses': clause_analyses,\n",
    "            'parties': parties,\n",
    "            'high_risk_areas': risk_assessment.high_risk_areas\n",
    "        }\n",
    "        \n",
    "        evaluation = await self.evaluator.analyze(preliminary_analysis)\n",
    "        \n",
    "        # Step 5: Generate final report\n",
    "        print(\"üìù Generating final report...\")\n",
    "        analysis_components = {\n",
    "            'is_contract': is_contract,\n",
    "            'confidence': confidence,\n",
    "            'risk_assessment': risk_assessment,\n",
    "            'insights': insights,\n",
    "            'parties': parties,\n",
    "            'clause_analyses': clause_analyses,\n",
    "            'evaluation': evaluation\n",
    "        }\n",
    "        \n",
    "        final_analysis = await self.reporter.analyze(analysis_components)\n",
    "        \n",
    "        print(\"‚úÖ Analysis complete!\")\n",
    "        \n",
    "        return asdict(final_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c23f2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synchronous version for easier Jupyter usage\n",
    "def sync_test_contract_analyzer():\n",
    "    \"\"\"Synchronous test function for Jupyter notebooks\"\"\"\n",
    "    \n",
    "    # Initialize your Azure OpenAI LLM\n",
    "    llm = AzureChatOpenAI(\n",
    "    deployment_name=\"neostats_hackathon_api_v1\",\n",
    "    model=\"gpt-oss-120b\",                         \n",
    "    temperature=0,\n",
    "    api_version=\"2024-05-01-preview\",             \n",
    "    api_key=\"Azure_Open_API\",                      # Replace with your actual key\n",
    "    azure_endpoint=\"https://neoaihackathon.services.ai.azure.com/models/chat/completions?api-version=2024-05-01-preview\" \n",
    "    )\n",
    "    \n",
    "    # Initialize the existing contract parser\n",
    "    # Assuming ContractParser is available in your environment\n",
    "    contract_parser = ContractParser()\n",
    "    \n",
    "    # Initialize the multi-agent analyzer\n",
    "    analyzer = MultiAgentContractAnalyzer(llm, contract_parser)\n",
    "    \n",
    "    # Sample contract for testing\n",
    "    sample_contract = contract_document.copy()\n",
    "    \n",
    "    # Run the analysis using asyncio.create_task for Jupyter compatibility\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    \n",
    "    loop = asyncio.new_event_loop()\n",
    "    asyncio.set_event_loop(loop)\n",
    "    \n",
    "    try:\n",
    "        result = loop.run_until_complete(analyzer.analyze_contract(sample_contract))\n",
    "        \n",
    "        # Display results in a dashboard-friendly format\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"CONTRACT ANALYSIS DASHBOARD DATA\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        if 'error' in result:\n",
    "            print(f\"‚ùå Error: {result['error']}\")\n",
    "            print(f\"Confidence: {result['confidence']}\")\n",
    "            print(f\"Reasoning: {result['reasoning']}\")\n",
    "            return result\n",
    "        \n",
    "        print(f\"\\nüìä EXECUTIVE SUMMARY:\")\n",
    "        print(f\"Document ID: {result['document_id']}\")\n",
    "        print(f\"Analysis Time: {result['timestamp']}\")\n",
    "        print(f\"Overall Risk Score: {result['risk_assessment']['overall_risk_score']:.2f}\")\n",
    "        print(f\"Contract Type: {result['insights']['contract_type']}\")\n",
    "        \n",
    "        print(f\"\\nüéØ KEY METRICS:\")\n",
    "        risk_dist = result['risk_assessment']['risk_distribution']\n",
    "        print(f\"High Risk Clauses: {risk_dist['HIGH']}\")\n",
    "        print(f\"Medium Risk Clauses: {risk_dist['MEDIUM']}\")\n",
    "        print(f\"Low Risk Clauses: {risk_dist['LOW']}\")\n",
    "        \n",
    "        print(f\"\\nüë• PARTIES INVOLVED:\")\n",
    "        for i, party in enumerate(result['parties'], 1):\n",
    "            print(f\"{i}. {party['name']} ({party['role']})\")\n",
    "        \n",
    "        print(f\"\\n‚ö†Ô∏è  HIGH RISK AREAS:\")\n",
    "        for area in result['risk_assessment']['high_risk_areas']:\n",
    "            print(f\"‚Ä¢ {area}\")\n",
    "        \n",
    "        print(f\"\\nüí° KEY RECOMMENDATIONS:\")\n",
    "        for rec in result['recommendations']:\n",
    "            print(f\"‚Ä¢ {rec}\")\n",
    "        \n",
    "        # Export as JSON for dashboard integration\n",
    "        with open(f\"contract_analysis_{result['document_id']}.json\", \"w\") as f:\n",
    "            json.dump(result, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"\\nüíæ Full analysis exported to: contract_analysis_{result['document_id']}.json\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during analysis: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        loop.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bfe1d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
